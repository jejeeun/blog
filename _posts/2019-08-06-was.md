---
title: WAS 파일 I/O 병목 해결을 위한 Hazelcast 분산 로깅 시스템 구축기
description: 전사 통합 로그 관리와 파일 I/O 성능 개선을 위한 분산 로깅 아키텍처 설계 및 구현 경험
author: backend-engineer
date: 2024-06-15 14:30:00 +0900
categories: [Backend, Architecture]
tags: [hazelcast, logging, spring-boot, performance, distributed-system]
math: false
mermaid: true
# image:
#   path: /assets/img/hazelcast-logging.png
#   alt: Hazelcast 분산 로깅 시스템 아키텍처
---

> **TL;DR**: 동기 파일 로깅의 컨텍스트 스위치 병목을 해결하기 위해 AsyncAppender와 Hazelcast ReliableTopic을 비교 분석하고, Redis Pub/Sub의 메시지 유실 문제와 해결책을 실무 관점에서 정리했습니다.

---

## 들어가며

멀티모듈 환경에서 로깅 시스템을 설계할 때 파일 I/O로 인한 성능 병목은 예측 가능한 문제 중 하나입니다. 특히 높은 트래픽이 예상되는 시스템에서는 로그 처리가 전체 애플리케이션 성능에 미치는 영향을 사전에 고려해야 했습니다.

여기에 더해 고객사에서 **중앙집중식 로그 관리**에 대한 요구사항이 추가되면서 단순한 성능 최적화를 넘어 분산 로깅 아키텍처에 대한 전면적인 검토가 필요했습니다.

기존의 각 서비스별 개별 파일 로깅으로는 고객사의 중앙집중식 로그 요구사항을 만족할 수 없었고, 동시에 성능 저하 없이 이를 구현하는 것이 새로운 과제가 되었습니다. 이번 포스트에서는 실제 운영 환경에서 겪은 로깅 병목 문제와 고객사 요구사항을 모두 만족시키기 위해 검토한 여러 아키텍처의 장단점을 공유하고자 합니다.

## 1. 문제의 시작: 왜 파일 로깅이 병목이 될까?

### 동기 파일 I/O의 숨겨진 비용

```java
// 일반적인 동기 로깅 - 문제의 시작점
logger.info("사용자 로그인: userId={}", userId);
// 이 한 줄이 실행되는 동안...
// 1. 파일 시스템 호출 (disk I/O)
// 2. 디스크 응답 대기 (10-50ms blocking)
// 3. 스레드 블록 → OS 컨텍스트 스위치
// 4. CPU 캐시 무효화 및 TLB flush
```

**컨텍스트 스위치(Context Switch)**: 운영체제가 현재 실행 중인 프로세스나 스레드를 일시 중단하고 다른 프로세스/스레드로 CPU 제어권을 전환하는 과정입니다. 이 과정에서 레지스터 상태 저장/복원, 메모리 맵 변경 등의 오버헤드가 발생합니다.

실제 성능 측정 결과:
- 동기 로깅: 평균 응답시간 150ms, TPS 200
- 비동기 로깅 적용 후: 평균 응답시간 45ms, TPS 800

### 병목의 과학적 근거

```
디스크 I/O 지연(10-50ms) → 스레드 블록 → OS 스케줄러 개입 
→ 컨텍스트 스위치 → CPU 캐시 미스 → 성능 저하
```

### 멀티프로세스 vs 멀티스레드 vs Node.js의 접근 방식

**멀티스레드 (Java/Spring Boot)**:
- 여러 스레드가 하나의 프로세스 메모리 공간을 공유
- 컨텍스트 스위치 비용이 상대적으로 낮음 (메모리 맵 유지)
- 하지만 I/O 블로킹 시 스레드 수만큼 컨텍스트 스위치 발생

**멀티프로세스**:
- 각 프로세스가 독립된 메모리 공간
- 컨텍스트 스위치 비용이 높음 (전체 메모리 맵 교체)
- 프로세스 간 통신(IPC) 오버헤드 추가

**Node.js (싱글 스레드 + 이벤트 루프)**:
```javascript
// Node.js는 I/O를 논블로킹으로 처리
fs.writeFile('app.log', logMessage, (err) => {
    if (err) console.error(err);
    // 콜백은 이벤트 루프에서 비동기 실행
});
// 메인 스레드는 즉시 다음 작업 진행
```
- 싱글 스레드로 컨텍스트 스위치 최소화
- 하지만 CPU 집약적 작업에는 부적합
- 확장성을 위해 cluster 모듈로 멀티프로세스 활용

**Java 환경에서의 딜레마**:
Java/Spring Boot는 멀티스레드 기반이므로 Node.js의 이벤트 루프 방식을 직접 적용하기 어렵습니다. 대신 **비동기 큐 + 백그라운드 스레드** 패턴으로 유사한 효과를 얻을 수 있습니다.

이러한 병목은 특히 고트래픽 환경에서 exponential하게 악화됩니다.

## 2. 해결책 탐색: 비동기 로깅 아키텍처 비교

### 2.1 Logback AsyncAppender: JVM 내부 큐 방식

**AsyncAppender**: Logback에서 제공하는 비동기 로깅 appender로, 내부적으로 BlockingQueue를 사용하여 로그 이벤트를 별도 스레드에서 처리합니다.

```xml
<!-- logback-spring.xml -->
<configuration>
    <appender name="FILE" class="ch.qos.logback.core.FileAppender">
        <file>application.log</file>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>
    
    <appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <queueSize>10000</queueSize>
        <discardingThreshold>0</discardingThreshold>
        <neverBlock>false</neverBlock>
        <appender-ref ref="FILE" />
    </appender>
    
    <root level="INFO">
        <appender-ref ref="ASYNC" />
    </root>
</configuration>
```

**장점:**
- 구현 복잡도 낮음
- JVM 내부 메모리 큐로 지연시간 최소
- Spring Boot와 완벽 호환

**단점:**
- 단일 노드 제한 (모듈마다 로그 분산 필요)
- JVM 재시작 시 큐 내 메시지 손실 가능
- 메모리 사용량 증가

### 2.2 Hazelcast Pub/Sub: 분산 메시징 방식

**Hazelcast**: 인메모리 데이터 그리드 플랫폼으로, 분산 컴퓨팅과 캐싱을 지원합니다. ReliableTopic은 메시지 유실을 방지하는 pub/sub 메커니즘을 제공합니다.

### 실제 구현: Logback Custom Appender + Hazelcast 통합

**1. Logback 설정 (boot-common/logback-spring.xml)**

```xml
<configuration scan="false" scanPeriod="15 seconds">
    <springProperty name="profile" source="spring.profiles.active" defaultValue="default"/>
    <springProperty name="appName" source="spring.application.name" defaultValue="unknown"/>
    
    <!-- Console Appender -->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <charset>UTF-8</charset>
            <pattern>%d{yyyy-MM-dd HH:mm:ss} %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>
    
    <!-- Cache Appender - Hazelcast로 분산 로깅 -->
    <appender name="LOG_STASH" class="com.asset.logging.CacheAppender">
        <module>${appName}</module>
        <profile>${profile}</profile>
        <logTopic>${log.topic-name:-LOG}</logTopic>
    </appender>
    
    <springProfile name="prod">
        <root level="INFO">
            <appender-ref ref="CONSOLE" />
            <appender-ref ref="LOG_STASH" />
        </root>
    </springProfile>
</configuration>
```

**2. Custom CacheAppender 구현**

```java
@Slf4j
public class CacheAppender extends AppenderBase<ILoggingEvent> {

    private static final ObjectMapper objectMapper = new ObjectMapper();
    private CacheProxy<String, String> cacheProxy;
    private String module;
    private String profile;
    private String logTopic;
    private boolean isInitialized = false;

    // 성능 최적화: ThreadLocal로 Map 재사용
    private static final ThreadLocal<Map<String, Object>> threadLocalLogMap = 
        ThreadLocal.withInitial(ConcurrentHashMap::new);

    // Logback XML에서 동적 주입
    public void setModule(String module) { this.module = module; }
    public void setProfile(String profile) { this.profile = profile; }
    public void setLogTopic(String logTopic) { this.logTopic = logTopic; }
    
    public void setCacheProxy(CacheProxy<String, String> cacheProxy) {
        this.cacheProxy = cacheProxy;
        this.isInitialized = true;
    }

    @Override
    protected void append(ILoggingEvent event) {
        if (!isInitialized) return;

        try {
            Map<String, Object> logEntry = threadLocalLogMap.get();
            logEntry.clear();

            logEntry.put("timestamp", event.getTimeStamp());
            logEntry.put("level", event.getLevel().toString());
            logEntry.put("thread", event.getThreadName());
            logEntry.put("logger", event.getLoggerName());
            logEntry.put("message", event.getFormattedMessage());
            logEntry.put("profile", profile);
            logEntry.put("module", module);

            String jsonLog = objectMapper.writeValueAsString(logEntry);
            cacheProxy.publishJsonString(logTopic, jsonLog);

        } catch (Exception e) {
            // StackOverflow 방지를 위해 System.err 사용
            System.err.println("[CacheAppender] 로그 전송 실패: " + e.getMessage());
        }
    }
}
```

**3. Hazelcast 메시지 서비스 구현**

```java
@Service
@Conditional(HazelcastPubSubCondition.class)
public class HazelcastMessageService<T extends Serializable> implements CacheMessageService<T> {

    private static final ObjectMapper objectMapper = new ObjectMapper();
    private final HazelcastInstance hazelcastInstance;
    private final HazelcastConfigManager<T> ringBufferManager;
    private final ConcurrentHashMap<String, UUID> listenerMap = new ConcurrentHashMap<>();

    @Override
    public void publish(String channel, T message) {
        // 발송자 정보 포함한 MessageWrapper 생성
        String senderId = hazelcastInstance.getCluster().getLocalMember().getUuid().toString();
        MessageWrapper<T> wrapped = new MessageWrapper<>(senderId, message);

        // 1. 실시간 전송 (ITopic)
        ITopic<MessageWrapper<T>> topic = hazelcastInstance.getTopic(channel);
        topic.publish(wrapped);

        // 2. Ringbuffer 저장 (과거 메시지 재생용)
        Ringbuffer<MessageWrapper<T>> ringbuffer = 
            ringBufferManager.getRingbuffer(channel + "-ringbuffer");
        ringbuffer.addAsync(wrapped, OverflowPolicy.OVERWRITE);
    }

    @Override
    public void subscribe(String channel, CacheMessageAdapter<T> delegate) {
        // 1. 과거 메시지 재생 (Ringbuffer에서)
        replayFromRingbuffer(channel, delegate);

        // 2. 실시간 수신 설정
        String localMemberId = hazelcastInstance.getCluster().getLocalMember().getUuid().toString();
        ITopic<MessageWrapper<T>> topic = hazelcastInstance.getTopic(channel);
        
        // 자기 메시지 필터링하는 래핑된 delegate
        CacheMessageAdapter<MessageWrapper<T>> wrappedDelegate = wrapper -> {
            if (!localMemberId.equals(wrapper.getSenderId())) {
                delegate.handleMessage(wrapper.getPayload());
            }
        };

        UUID listenerId = topic.addMessageListener(
            new HazelcastMessageListener<>(ringBufferManager, channel, wrappedDelegate, localMemberId)
        );
        
        listenerMap.put(channel, listenerId);
    }

    @Override
    public void publishJsonString(String channel, T message) {
        try {
            String senderId = hazelcastInstance.getCluster().getLocalMember().getUuid().toString();

            Map<String, Object> jsonPayload = Map.of(
                "senderId", senderId,
                "timestamp", System.currentTimeMillis(),
                "payload", message
            );

            String json = objectMapper.writeValueAsString(jsonPayload);
            ITopic<String> topic = hazelcastInstance.getTopic(channel);
            topic.publish(json);

        } catch (JsonProcessingException e) {
            log.warn("JSON 직렬화 실패", e);
        }
    }

    private void replayFromRingbuffer(String channel, CacheMessageAdapter<T> delegate) {
        Ringbuffer<MessageWrapper<T>> ringbuffer = 
            ringBufferManager.getRingbuffer(channel + "-ringbuffer");

        try {
            long head = ringbuffer.headSequence();
            long tail = ringbuffer.tailSequence();
            long fromSequence = Math.max(tail - 100, head); // 최근 100개
            String localMemberId = hazelcastInstance.getCluster().getLocalMember().getUuid().toString();

            ringbuffer.readManyAsync(fromSequence, 1, 100, null)
                .whenComplete((resultSet, throwable) -> {
                    if (throwable != null) {
                        log.warn("Ringbuffer read 실패 [{}]", channel, throwable);
                        return;
                    }

                    resultSet.forEach(msg -> {
                        if (!localMemberId.equals(msg.getSenderId())) {
                            delegate.handleMessage(msg.getPayload());
                        }
                    });
                });
        } catch (Exception e) {
            log.warn("Ringbuffer 초기화 중 오류 [{}]", channel, e);
        }
    }

    @Override
    public void unsubscribe(String channel) {
        UUID listenerId = listenerMap.remove(channel);
        if (listenerId != null) {
            ITopic<MessageWrapper<T>> topic = hazelcastInstance.getTopic(channel);
            topic.removeMessageListener(listenerId);
        }
    }
}
```

**4. 멀티모듈 아키텍처와 통합**

```
platform-backend/
├── boot-common/                    # 공통 모듈
│   ├── logback-spring.xml         # 통합 로깅 설정
│   └── CacheAppender.java         # 커스텀 Appender
├── admin-service/                 # 각 마이크로서비스
├── authorization-service/
├── gateway-service/
└── cache-reactive-service/        # 로그 수집 서비스
```

모든 서비스가 `boot-common`을 의존하므로 **일관된 로깅 정책**이 자동 적용됩니다. 각 서비스의 로그가 자동으로 중앙집중식으로 수집되는 구조입니다.

### 2.3 Redis Pub/Sub의 함정과 대안

처음에는 Redis Pub/Sub을 고려했지만, 치명적인 한계를 발견했습니다.

```bash
# Redis Pub/Sub - 메시지 유실 시나리오
127.0.0.1:6379> PUBLISH logs "중요한 로그 메시지"
(integer) 0  # 구독자가 없어서 메시지 소멸!

# 네트워크 파티션 발생 시
# 구독 연결이 끊어지면 그 순간의 모든 메시지 유실
```

**Redis Pub/Sub 유실 원인:**
- 메시지가 메모리에만 존재 (영속성 없음)
- 구독자가 없으면 즉시 소멸
- `noeviction` 정책은 key eviction에만 적용되어 pub/sub 메시지에는 무관

**대안: Redis Streams**

```bash
# Redis Streams - 영속성과 재처리 보장
127.0.0.1:6379> XADD app-logs * level INFO msg "사용자 로그인" service "user-api"
"1721808000000-0"

# Consumer Group으로 at-least-once 보장
127.0.0.1:6379> XREADGROUP GROUP log-processors consumer1 STREAMS app-logs >
```

```java
@Service
public class RedisStreamsLogPublisher {
    
    private final RedisTemplate<String, Object> redisTemplate;
    private static final String STREAM_KEY = "app-logs";
    
    public void publishLog(LogMessage logMessage) {
        Map<String, Object> fields = Map.of(
            "timestamp", logMessage.getTimestamp(),
            "level", logMessage.getLevel(),
            "message", logMessage.getMessage(),
            "service", logMessage.getServiceName()
        );
        
        redisTemplate.opsForStream()
            .add(STREAM_KEY, fields);
    }
}
```

## 3. 로그 구독자(log-subscriber) 아키텍처

실제 중앙집중식 로깅의 핵심은 **구독자 시스템**입니다. 모든 마이크로서비스에서 발송된 로그를 수신하여 적절히 처리하는 독립적인 서비스를 구현했습니다.

### 3.1 메인 구독자 설정

```java
@Component
@RequiredArgsConstructor
public class LogSubscriber {

    private final CachePubSubFactory factory;
    private final CacheMessageAdapter<String> logHandler;

    @Value("${log.topic-name:LOG}")
    private String logTopic;

    @PostConstruct
    public void subscribe() {
        CacheMessageService<String> service = factory.createPubSubService(String.class);
        service.subscribe(logTopic, logHandler);
        log.info("LogSubscriber: '{}' 채널 구독 시작", logTopic);
    }
}
```

### 3.2 로그 레벨별 분산 처리: LogDispatcher

**문제**: 모든 로그를 단일 스레드로 처리하면 ERROR 로그가 INFO 로그에 밀려 지연될 수 있습니다.

**해결**: 로그 레벨별로 큐를 분리하여 우선순위 처리를 구현했습니다.

```java
@Component
@RequiredArgsConstructor
public class LogDispatcher {

    private final LogFileAdapter logFileAdapter;

    // 로그 레벨별 큐 크기 최적화
    private final Map<String, BlockingQueue<String>> levelQueues = Map.of(
            "ERROR", new ArrayBlockingQueue<>(4096),  // 에러는 큰 버퍼
            "WARN", new ArrayBlockingQueue<>(2048),   // 워닝은 중간
            "INFO", new ArrayBlockingQueue<>(4096),   // 정보성 로그
            "DEBUG", new ArrayBlockingQueue<>(8192)   // 디버그는 가장 많음
    );

    private final List<Thread> workerThreads = new ArrayList<>();

    @PostConstruct
    public void startWorkers() {
        levelQueues.forEach((level, queue) -> {
            Thread worker = new Thread(() -> {
                while (!Thread.currentThread().isInterrupted()) {
                    try {
                        String msg = queue.take(); // Blocking
                        logFileAdapter.handleMessage(msg);
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                        break;
                    } catch (Exception ex) {
                        log.error("로그 처리 중 예외 발생 (level: {})", level, ex);
                    }
                }
            });
            worker.setName("log-worker-" + level);
            worker.setDaemon(true);
            worker.start();
            workerThreads.add(worker);
        });
        log.info("LogDispatcher: {} worker threads started", workerThreads.size());
    }

    @PreDestroy
    public void shutdown() {
        log.info("LogDispatcher: Initiating graceful shutdown...");
        workerThreads.forEach(Thread::interrupt);
        // 최대 1초 대기 후 강제 종료
        workerThreads.forEach(worker -> {
            try { worker.join(1000); } 
            catch (InterruptedException e) { Thread.currentThread().interrupt(); }
        });
    }
}
```

### 3.3 동적 로그 파일 생성: LogFileAdapter

**핵심 아이디어**: 각 마이크로서비스의 profile/module/level에 따라 **동적으로 로거를 생성**하여 파일을 분리합니다.

```java
@Component
@RequiredArgsConstructor
public class LogFileAdapter implements CacheMessageAdapter<String> {
    
    private static final DateTimeFormatter formatter = 
        DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");
    private static final ObjectMapper objectMapper = new ObjectMapper();
    
    // 동적 생성된 Logger 캐싱
    private final Map<String, Logger> loggerCache = new ConcurrentHashMap<>();

    @Value("${log.file.max-history:7}")
    private int maxHistory;

    @Value("${log.file.base-path:../../logs/}")
    private String baseLogPath;

    @Override
    public void handleMessage(String message) {
        try {
            Map<String, Object> logMap = objectMapper.readValue(message, Map.class);

            String profile = (String) logMap.getOrDefault("profile", "default");
            String module = (String) logMap.getOrDefault("module", "unknown");
            String level = ((String) logMap.getOrDefault("level", "INFO")).toUpperCase();
            Long timestamp = ((Number) logMap.get("timestamp")).longValue();
            String thread = (String) logMap.getOrDefault("thread", "main");
            String loggerName = (String) logMap.getOrDefault("logger", "anonymous");
            String logMessage = (String) logMap.getOrDefault("message", "");

            String formattedTime = formatter.format(
                Instant.ofEpochMilli(timestamp).atZone(ZoneId.of("Asia/Seoul"))
            );

            // 통합 로그 포맷
            String formattedLog = String.format(
                "[%s] [%s] [%s] [%s] [%s] [%s] - %s",
                profile, module, formattedTime, level, thread, loggerName, logMessage
            );

            Logger moduleLogger = getOrCreateLogger(profile, module, level);

            // 로그 레벨에 따른 적절한 메서드 호출
            switch (level) {
                case "ERROR" -> moduleLogger.error(formattedLog);
                case "WARN" -> moduleLogger.warn(formattedLog);
                case "DEBUG" -> moduleLogger.debug(formattedLog);
                default -> moduleLogger.info(formattedLog);
            }

        } catch (Exception e) {
            log.warn("로그 파싱 실패: {}", message, e);
        }
    }

    /**
     * 동적 Logger 생성 로직
     * 경로: ../../logs/{module}/{profile}/{yyyy/MMdd}/{Level}_yyyyMMdd.log.gz
     */
    private Logger getOrCreateLogger(String profile, String module, String level) {
        String loggerName = "module." + module + "." + profile + "." + level;
        
        return loggerCache.computeIfAbsent(loggerName, name -> {
            LoggerContext context = (LoggerContext) LoggerFactory.getILoggerFactory();
            Logger logger = context.getLogger(name);

            // 파일 경로 동적 생성
            String basePath = baseLogPath + module + "/" + profile + "/";
            String fileNamePattern = basePath + "%d{yyyy/MMdd}/" + 
                level.substring(0,1).toUpperCase() + level.substring(1).toLowerCase() + 
                "_%d{yyyyMMdd}.log.gz";

            // RollingFileAppender 설정
            RollingFileAppender fileAppender = new RollingFileAppender();
            fileAppender.setContext(context);
            fileAppender.setName("FILE-" + module + "-" + profile + "-" + level);

            TimeBasedRollingPolicy rollingPolicy = new TimeBasedRollingPolicy();
            rollingPolicy.setContext(context);
            rollingPolicy.setParent(fileAppender);
            rollingPolicy.setFileNamePattern(fileNamePattern);
            rollingPolicy.setMaxHistory(maxHistory); // 7일 보관
            fileAppender.setRollingPolicy(rollingPolicy);
            rollingPolicy.start();

            PatternLayoutEncoder encoder = new PatternLayoutEncoder();
            encoder.setContext(context);
            encoder.setPattern("%msg%n"); // 이미 포맷된 메시지 그대로 출력
            encoder.start();

            fileAppender.setEncoder(encoder);
            fileAppender.start();

            // AsyncAppender로 논블로킹 처리
            AsyncAppender asyncAppender = new AsyncAppender();
            asyncAppender.setContext(context);
            asyncAppender.setName("ASYNC-" + module + "-" + profile + "-" + level);
            asyncAppender.setQueueSize(16384);
            asyncAppender.setDiscardingThreshold(90); // 90% 찰 때까지 버퍼링
            asyncAppender.addAppender(fileAppender);
            asyncAppender.start();

            logger.setAdditive(false); // 부모 Logger로 전파 방지
            logger.addAppender(asyncAppender);

            return logger;
        });
    }
}
```

### 3.4 Hazelcast 메시지 리스너와 실패 복구

```java
@RequiredArgsConstructor
public class HazelcastMessageListener<T> implements MessageListener<T> {

    private final HazelcastConfigManager<T> ringbufferConfigManager;
    private final String topicName;
    private final CacheMessageAdapter<T> delegate;
    private final String localMemberId;
    
    private final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(2);

    @Override
    public void onMessage(Message<T> message) {
        T payload = message.getMessageObject();

        try {
            delegate.handleMessage(payload); // 정상 처리
        } catch (Exception e) {
            log.warn("메시지 처리 실패 [{}]: {}", topicName, e.getMessage());
            
            // 실패한 메시지를 RingBuffer에 저장하고 재시도 스케줄링
            String ringBufferName = topicName + "-ringbuffer";
            Ringbuffer<T> ringbuffer = ringbufferConfigManager.getRingbuffer(ringBufferName);
            
            ringbuffer.addAsync(payload, OverflowPolicy.OVERWRITE)
                .thenAccept(seq -> {
                    log.info("실패 메시지 RingBuffer 저장 [{}], seq={}", topicName, seq);
                    scheduleRetry(ringbuffer, seq, topicName, 30L); // 30초 후 재시도
                });
        }
    }

    private void scheduleRetry(Ringbuffer<T> ringbuffer, long sequence, String topicName, long retryPeriod) {
        scheduler.scheduleAtFixedRate(() -> {
            try {
                T payload;
                try {
                    payload = ringbuffer.readOne(sequence);
                } catch (StaleSequenceException stale) {
                    // Sequence가 너무 오래되어 RingBuffer에서 사라진 경우
                    long safeSeq = ringbuffer.headSequence();
                    log.warn("Stale sequence [{}], 최신 위치로 재설정 [{}]", sequence, safeSeq);
                    payload = ringbuffer.readOne(safeSeq);
                }

                delegate.handleMessage(payload);
                log.info("재시도 성공 [{}]: {}", topicName, payload);
                throw new CancellationException(); // 성공 시 재시도 중단

            } catch (CancellationException stop) {
                throw stop; // 정상 종료
            } catch (ClassCastException castEx) {
                log.error("타입 불일치로 재시도 중단 [{}]: {}", topicName, castEx.getMessage());
                throw new CancellationException();
            } catch (Exception e) {
                log.warn("재시도 실패 [{}]: {}", topicName, e.getMessage());
                // 계속 재시도
            }
        }, 0, retryPeriod, TimeUnit.SECONDS);
    }
}
```

### 3.5 최종 로그 파일 구조

구독자를 통해 생성되는 로그 파일 구조는 다음과 같습니다:

```
logs/
├── admin-service/
│   ├── prod/
│   │   └── 2025/0714/
│   │       ├── Error_20250714.log.gz
│   │       ├── Warn_20250714.log.gz
│   │       └── Info_20250714.log.gz
│   └── dev/
├── gateway-service/
│   ├── prod/
│   └── dev/
└── authorization-service/
    ├── prod/
    └── dev/
```

각 로그 파일의 내용:
```
[prod] [admin-service] [2025-07-14 15:30:25] [INFO] [http-nio-8080-exec-1] [com.asset.admin.UserController] - 사용자 로그인 성공: userId=12345
[prod] [admin-service] [2025-07-14 15:30:26] [ERROR] [http-nio-8080-exec-2] [com.asset.admin.OrderService] - 주문 처리 실패: orderId=67890
```

## 4. 아키텍처 선택 기준과 실무 적용

### 우리 팀의 최종 선택: 하이브리드 아키텍처

**선택 이유**: 고객사의 중앙집중식 로그 요구사항 + 성능 최적화 + 운영 현실성의 균형

### 실제 운영 지표

**적용 전 (각 서비스별 파일 로깅)**
```
- 로그 파일: 12개 서비스 × 3개 환경 = 36개 디렉토리
- 로그 검색: SSH + grep 조합으로 수동 검색
- 장애 추적: 평균 15분 (여러 서비스 로그 수집 시간)
- 스토리지: 각 서버별 로컬 디스크 (백업 누락 위험)
```

**적용 후 (Hazelcast + log-subscriber)**
```
- 로그 파일: 체계적 구조 (../../logs/{module}/{profile}/{date}/{level}.log.gz)
- 로그 검색: 중앙 서버에서 통합 검색 가능
- 장애 추적: 평균 3분 (단일 위치에서 모든 로그 조회)
- 스토리지: 중앙집중 + 자동 압축 (7일 순환 보관)
```

## 5. 성능 영향과 운영 경험

### 체감 성능 개선

실제 운영에서 느낀 개선 효과:

```
기존 (동기 파일 로깅):
- 로그 많은 API 호출 시 눈에 띄는 지연
- CPU 사용률이 높을 때 응답 속도 저하 체감

적용 후 (비동기 + 분산 로깅):
- API 응답 속도 체감상 개선
- 로그량이 많아져도 서비스 성능에 영향 없음
- 분산 로깅 추가로 약간의 메모리 사용량 증가 (허용 범위)
```

### 실제 운영에서 중요했던 것들

**1. 로그 레벨별 처리 우선순위**
```java
// ERROR 로그가 INFO/DEBUG에 밀리지 않도록 큐 분리
private final Map<String, BlockingQueue<String>> levelQueues = Map.of(
    "ERROR", new ArrayBlockingQueue<>(4096),  // 에러는 충분한 버퍼
    "WARN", new ArrayBlockingQueue<>(2048),   
    "INFO", new ArrayBlockingQueue<>(4096),   
    "DEBUG", new ArrayBlockingQueue<>(8192)   // 디버그는 가장 많음
);
```
실제로 ERROR 로그가 DEBUG 로그에 밀려서 늦게 처리되는 것을 방지할 수 있었습니다.

**2. 메모리 사용량 관리**
```java
// Logger 무제한 생성 방지
private final Map<String, Logger> loggerCache = 
    Collections.synchronizedMap(new LinkedHashMap<String, Logger>(100, 0.75f, true) {
        @Override
        protected boolean removeEldestEntry(Map.Entry<String, Logger> eldest) {
            return size() > 100; // 100개 초과 시 오래된 Logger 제거
        }
    });
```
초기에는 Logger가 무제한 생성되어 메모리 문제가 있었지만, LRU 캐시로 해결했습니다.

### 실제 운영 중 발견한 이슈와 해결책

#### 이슈 1: Hazelcast 클러스터 스플릿 브레인
```yaml
# 해결: 최소 클러스터 크기 강제
hazelcast:
  cluster-name: logging-cluster
  properties:
    hazelcast.merge.first.run.delay.seconds: 5
    hazelcast.merge.next.run.delay.seconds: 5
  partition-group:
    enabled: true
    group-type: HOST_AWARE  # 같은 호스트 내 백업 방지
```

#### 이슈 2: log-subscriber OOM 발생
**원인**: 대량 로그 유입 시 LogFileAdapter의 동적 Logger 무제한 생성

```java
// 해결: Logger 캐시 크기 제한 + LRU 정책
private final Map<String, Logger> loggerCache = 
    Collections.synchronizedMap(new LinkedHashMap<String, Logger>(100, 0.75f, true) {
        @Override
        protected boolean removeEldestEntry(Map.Entry<String, Logger> eldest) {
            if (size() > 100) {
                // 오래된 Logger 정리
                Logger oldLogger = eldest.getValue();
                oldLogger.detachAndStopAllAppenders();
                return true;
            }
            return false;
        }
    });
```

#### 이슈 3: 로그 파일 디스크 풀
**원인**: 압축되지 않은 로그 파일 급속 증가

```java
// 해결: 실시간 압축 + 더 공격적인 순환 정책
TimeBasedRollingPolicy rollingPolicy = new TimeBasedRollingPolicy();
rollingPolicy.setFileNamePattern(fileNamePattern);
rollingPolicy.setMaxHistory(3); // 7일 → 3일로 단축
rollingPolicy.setTotalSizeCap(FileSize.valueOf("10GB")); // 전체 크기 제한 추가
```

## 6. 마무리: 현실적인 권고사항

### 단계별 적용 전략

1. **1단계**: 기존 시스템에 AsyncAppender 적용
2. **2단계**: 트래픽 증가 시 Hazelcast 도입 검토
3. **3단계**: 대규모 환경에서 Kafka 마이그레이션

### 체크리스트

- [ ] 현재 로깅량과 TPS 측정
- [ ] 메모리 사용량 모니터링 체계 구축
- [ ] 로그 유실 허용 수준 정의
- [ ] 장애 상황 대응 절차 수립
- [ ] 성능 임계치 알림 설정

---

**참고 자료**
- [Logback AsyncAppender 공식 문서](https://logback.qos.ch/manual/appenders.html#AsyncAppender)
- [Hazelcast ReliableTopic 가이드](https://docs.hazelcast.com/imdg/4.2/data-structures/reliable-topic)
- [Redis Streams vs Pub/Sub 비교](https://redis.io/docs/data-types/streams/)

실무에서는 완벽한 솔루션보다는 **현재 상황에 맞는 최적 선택**이 중요합니다. 여러분의 시스템 규모와 요구사항에 따라 적절한 방식을 선택하시기 바랍니다.

*이 글이 도움이 되셨다면 댓글로 여러분의 경험도 공유해 주세요! 🚀*