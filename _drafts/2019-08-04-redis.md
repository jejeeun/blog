---
title: Redis í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ë¶„ì‚° ìºì‹œ ì‹œìŠ¤í…œ êµ¬ì¶•ê¸° - Dockerì™€ DataFetcher íŒ¨í„´ ì‹¤ë¬´ ì ìš©
date: 2025-07-13 10:00:00 +0900
categories: [Backend, Infrastructure]
tags: [redis, docker, cluster, predixy, spring-boot, performance, devops]
---

## í•µì‹¬ ìš”ì•½

MSA í™˜ê²½ì—ì„œ Redis í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ë¶„ì‚° ìºì‹œ ì‹œìŠ¤í…œì„ Docker Composeë¡œ êµ¬ì¶•í•˜ê³ , DataFetcher íŒ¨í„´ì„ í™œìš©í•œ ìë™ DB ì—°ë™ ì‹œìŠ¤í…œì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. Predixy í”„ë¡ì‹œ ë„ì…ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ë³µì¡ì„±ì„ ì œê±°í•˜ê³ , StringRedisSerializer ìµœì í™”ë¥¼ í†µí•´ ìš©ëŸ‰ì„ 30% ì ˆì•½í–ˆìœ¼ë©°, í‰ê·  ì‘ë‹µì‹œê°„ì„ 200msì—ì„œ 50msë¡œ ë‹¨ì¶•(75% í–¥ìƒ)í•˜ì—¬ ìºì‹œ íˆíŠ¸ìœ¨ 85% ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

## 1. í”„ë¡œì íŠ¸ ë°°ê²½ ë° ìš”êµ¬ì‚¬í•­

### 1.1 ê¸°ì¡´ ì‹œìŠ¤í…œì˜ í•œê³„

```java
// ê¸°ì¡´ ë‹¨ìˆœ ìºì‹œ ì‚¬ìš© ë°©ì‹ì˜ ë¬¸ì œì 
@Service
public class ProductService {
    
    @Autowired
    private ProductRepository productRepository;
    
    public Product getProduct(String productId) {
        // ë§¤ë²ˆ DB ì§ì ‘ ì¡°íšŒ - ì„±ëŠ¥ ë³‘ëª©
        Product product = productRepository.findById(productId);
        
        if (product == null) {
            throw new EntityNotFoundException("Product not found: " + productId);
        }
        
        return product;
    }
}
```

**ì‹¤ë¬´ì—ì„œ ë°œìƒí•œ ë¬¸ì œë“¤:**
- **DB ë¶€í•˜ ì§‘ì¤‘**: ë™ì¼ ìƒí’ˆ ì¡°íšŒ ì‹œ ë§¤ë²ˆ DB ì¿¼ë¦¬ ë°œìƒ
- **ì‘ë‹µ ì§€ì—°**: í‰ê·  ì‘ë‹µì‹œê°„ 200msë¡œ ëª©í‘œ 50ms ë¯¸ë‹¬
- **í™•ì¥ì„± í•œê³„**: íŠ¸ë˜í”½ ì¦ê°€ ì‹œ DB ë³‘ëª©ìœ¼ë¡œ ì‹œìŠ¤í…œ ì „ì²´ ì„±ëŠ¥ ì €í•˜
- **ê°œë°œ ë³µì¡ë„**: ê° ì„œë¹„ìŠ¤ë§ˆë‹¤ ìºì‹œ ë¡œì§ ì¤‘ë³µ êµ¬í˜„

### 1.2 í•´ê²° ëª©í‘œ

1. **ì„±ëŠ¥ í–¥ìƒ**: í‰ê·  ì‘ë‹µì‹œê°„ 200ms â†’ 50ms ë‹¬ì„±
2. **DB ë¶€í•˜ ê°ì†Œ**: ë™ì¼ ë°ì´í„° ë°˜ë³µ ì¡°íšŒ 70% ê°ì†Œ
3. **í™•ì¥ì„± í™•ë³´**: Redis í´ëŸ¬ìŠ¤í„°ë¡œ ìˆ˜í‰ í™•ì¥ ì§€ì›
4. **ê°œë°œ íš¨ìœ¨ì„±**: íˆ¬ëª…í•œ ìºì‹œ ì²˜ë¦¬ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì§‘ì¤‘

## 2. Redis í´ëŸ¬ìŠ¤í„° êµ¬ì¶•

### 2.1 Docker Compose ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ì„¤ê³„

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Master ë…¸ë“œ 3ê°œ
  redis-node1:
    image: redis:7-alpine
    container_name: redis-node1
    ports:
      - "6300:6300"
      - "16300:16300"  # cluster bus port
    volumes:
      - ./redis_6300.conf:/usr/local/etc/redis/redis.conf
      - redis_node1_data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - redis-cluster
    restart: unless-stopped
    
  redis-node2:
    image: redis:7-alpine
    container_name: redis-node2
    ports:
      - "6301:6301"
      - "16301:16301"
    volumes:
      - ./redis_6301.conf:/usr/local/etc/redis/redis.conf
      - redis_node2_data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - redis-cluster
    restart: unless-stopped
    
  redis-node3:
    image: redis:7-alpine
    container_name: redis-node3
    ports:
      - "6302:6302"
      - "16302:16302"
    volumes:
      - ./redis_6302.conf:/usr/local/etc/redis/redis.conf
      - redis_node3_data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - redis-cluster
    restart: unless-stopped

  # Replica ë…¸ë“œ 3ê°œ
  redis-node4:
    image: redis:7-alpine
    container_name: redis-node4
    ports:
      - "6400:6400"
      - "16400:16400"
    volumes:
      - ./redis_6400.conf:/usr/local/etc/redis/redis.conf
      - redis_node4_data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - redis-cluster
    restart: unless-stopped
      
  redis-node5:
    image: redis:7-alpine
    container_name: redis-node5
    ports:
      - "6401:6401"
      - "16401:16401"
    volumes:
      - ./redis_6401.conf:/usr/local/etc/redis/redis.conf
      - redis_node5_data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - redis-cluster
    restart: unless-stopped
      
  redis-node6:
    image: redis:7-alpine
    container_name: redis-node6
    ports:
      - "6402:6402"
      - "16402:16402"
    volumes:
      - ./redis_6402.conf:/usr/local/etc/redis/redis.conf
      - redis_node6_data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - redis-cluster
    restart: unless-stopped

  # Predixy í”„ë¡ì‹œ
  predixy:
    image: haandol/predixy:latest
    container_name: predixy
    ports:
      - "7617:7617"
    volumes:
      - ./predixy/predixy.conf:/etc/predixy/predixy.conf
    depends_on:
      - redis-node1
      - redis-node2
      - redis-node3
      - redis-node4
      - redis-node5
      - redis-node6
    networks:
      - redis-cluster
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "localhost", "-p", "7617", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  redis_node1_data:
  redis_node2_data:
  redis_node3_data:
  redis_node4_data:
  redis_node5_data:
  redis_node6_data:

networks:
  redis-cluster:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

### 2.2 Redis ë…¸ë“œ ì„¤ì • íŒŒì¼

```bash
# redis_6300.conf (ë§ˆìŠ¤í„° ë…¸ë“œ ì˜ˆì‹œ)
# ë„¤íŠ¸ì›Œí¬ ì„¤ì •
port 6300
bind 0.0.0.0
protected-mode no

# í´ëŸ¬ìŠ¤í„° ì„¤ì •
cluster-enabled yes
cluster-config-file nodes-6300.conf
cluster-node-timeout 15000
cluster-require-full-coverage no

# ë°ì´í„° ì˜ì†ì„± ì„¤ì •
appendonly yes
appendfilename "appendonly-6300.aof"
appendfsync everysec
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# ë³´ì•ˆ ì„¤ì •
requirepass password123!
masterauth password123!

# ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •
maxmemory 2gb
maxmemory-policy allkeys-lru
maxmemory-samples 5

# ë¡œê¹… ì„¤ì •
loglevel notice
logfile "/var/log/redis/redis-6300.log"

# ì„±ëŠ¥ íŠœë‹ ì„¤ì •
tcp-keepalive 300
timeout 0
tcp-backlog 511
databases 1

# í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„¤ì •
maxclients 10000

# ìŠ¬ë¡œìš° ë¡œê·¸ ì„¤ì •
slowlog-log-slower-than 10000
slowlog-max-len 128
```

### 2.3 í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# cluster.sh - Redis í´ëŸ¬ìŠ¤í„° ìë™ êµ¬ì„± ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸš€ Starting Redis Cluster Setup..."

# 1. Docker Compose ì‹¤í–‰
echo "ğŸ“¦ Starting Redis containers..."
docker-compose up -d

# 2. ì»¨í…Œì´ë„ˆ ì‹œì‘ ëŒ€ê¸°
echo "â³ Waiting for Redis nodes to start..."
for i in {1..30}; do
    if docker exec redis-node1 redis-cli -p 6300 -a "password123!" ping > /dev/null 2>&1; then
        echo "âœ… Redis nodes are ready!"
        break
    fi
    echo "   Attempt $i/30: Waiting for Redis nodes..."
    sleep 2
done

# 3. í´ëŸ¬ìŠ¤í„° ìƒì„±
echo "ğŸ”— Creating Redis cluster..."
docker exec -it redis-node1 redis-cli -a "password123!" --cluster create \
  redis-node1:6300 redis-node2:6301 redis-node3:6302 \
  redis-node4:6400 redis-node5:6401 redis-node6:6402 \
  --cluster-replicas 1 --cluster-yes

# 4. í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
echo "ğŸ“Š Checking cluster status..."
docker exec redis-node1 redis-cli -a "password123!" cluster nodes

echo "ğŸ‰ Redis cluster setup completed successfully!"

# 5. Predixy ì—°ê²° í…ŒìŠ¤íŠ¸
echo "ğŸ” Testing Predixy connection..."
if docker exec predixy redis-cli -h localhost -p 7617 ping > /dev/null 2>&1; then
    echo "âœ… Predixy proxy is working!"
else
    echo "âŒ Predixy proxy test failed!"
    exit 1
fi

echo "ğŸ“ˆ Cluster information:"
docker exec redis-node1 redis-cli -a "password123!" --cluster info redis-node1:6300

echo "ğŸš€ Setup completed! Redis cluster is ready for use."
echo "   - Cluster endpoint: localhost:7617 (via Predixy)"
echo "   - Direct nodes: localhost:6300-6302, localhost:6400-6402"
echo "   - Password: password123!"
```

### 2.4 ìš´ì˜ í™˜ê²½ ë°©í™”ë²½ ì„¤ì •

```bash
#!/bin/bash
# firewall-setup.sh - ìš´ì˜ í™˜ê²½ ë°©í™”ë²½ ì„¤ì •

echo "ğŸ”¥ Configuring firewall for Redis cluster..."

# Redis ë…¸ë“œ í¬íŠ¸ ê°œë°©
sudo firewall-cmd --zone=public --add-port=6300-6302/tcp --permanent
sudo firewall-cmd --zone=public --add-port=6400-6402/tcp --permanent

# Redis í´ëŸ¬ìŠ¤í„° ë²„ìŠ¤ í¬íŠ¸ ê°œë°©
sudo firewall-cmd --zone=public --add-port=16300-16302/tcp --permanent
sudo firewall-cmd --zone=public --add-port=16400-16402/tcp --permanent

# Predixy í”„ë¡ì‹œ í¬íŠ¸ ê°œë°©
sudo firewall-cmd --zone=public --add-port=7617/tcp --permanent

# ë°©í™”ë²½ ì„¤ì • ì ìš©
sudo firewall-cmd --reload

# ì„¤ì • í™•ì¸
echo "ğŸ“‹ Current firewall rules:"
sudo firewall-cmd --list-all

echo "âœ… Firewall configuration completed!"
```

## 3. Predixy í”„ë¡ì‹œ ì„¤ì • ë° ìµœì í™”

### 3.1 Predixy ì„¤ì • íŒŒì¼

```bash
# predixy/predixy.conf
# ê¸°ë³¸ ì„¤ì •
Name Predixy-Redis-Cluster
Bind 0.0.0.0:7617
WorkerThreads 8
MaxMemory 512M
ClientTimeout 600
LogRotate 1d

# ë¡œê·¸ ì„¤ì • (ìš´ì˜ í™˜ê²½ ìµœì í™”)
LogVerbSample 0
LogDebugSample 0
LogInfoSample 100000
LogNoticeSample 1
LogWarnSample 1
LogErrorSample 1

# Redis í´ëŸ¬ìŠ¤í„° ì„œë²„ í’€ ì„¤ì •
ClusterServerPool {
    # ì½ê¸° ìš°ì„ ìˆœìœ„ ì„¤ì •
    MasterReadPriority 70
    StaticSlaveReadPriority 60
    DynamicSlaveReadPriority 50
    
    # í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ì„¤ì •
    RefreshInterval 5
    ServerTimeout 3
    ServerFailureLimit 5
    ServerRetryTimeout 3
    KeepAlive 300
    
    # Redis ë…¸ë“œ ëª©ë¡
    Servers {
        + redis-node1:6300
        + redis-node2:6301
        + redis-node3:6302
        + redis-node4:6400
        + redis-node5:6401
        + redis-node6:6402
    }
}

# ì¸ì¦ ì„¤ì •
Authority {
    Auth password123!
}

# ì—°ê²° í’€ ìµœì í™”
Include common/cluster.conf

# ëª…ë ¹ì–´ í•„í„°ë§ (ë³´ì•ˆ)
Include common/latency.conf
```

### 3.2 Predixy ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```bash
# predixy-monitor.sh - Predixy ìƒíƒœ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash

PREDIXY_HOST="localhost"
PREDIXY_PORT="7617"
LOG_FILE="/var/log/predixy/monitor.log"

# í˜„ì¬ ì‹œê°„
TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')

# Predixy ì—°ê²° í…ŒìŠ¤íŠ¸
if redis-cli -h $PREDIXY_HOST -p $PREDIXY_PORT ping > /dev/null 2>&1; then
    PREDIXY_STATUS="UP"
else
    PREDIXY_STATUS="DOWN"
fi

# Predixy í†µê³„ ì •ë³´ ìˆ˜ì§‘
STATS=$(redis-cli -h $PREDIXY_HOST -p $PREDIXY_PORT info 2>/dev/null || echo "ERROR")

# Redis í´ëŸ¬ìŠ¤í„° ë…¸ë“œ ìƒíƒœ í™•ì¸
CLUSTER_STATUS=$(docker exec redis-node1 redis-cli -a "password123!" cluster info 2>/dev/null | grep cluster_state | cut -d: -f2)

# ë¡œê·¸ ê¸°ë¡
echo "[$TIMESTAMP] Predixy: $PREDIXY_STATUS, Cluster: $CLUSTER_STATUS" >> $LOG_FILE

# ìƒíƒœê°€ ì •ìƒì´ ì•„ë‹Œ ê²½ìš° ì•Œë¦¼
if [ "$PREDIXY_STATUS" != "UP" ] || [ "$CLUSTER_STATUS" != "ok" ]; then
    echo "ğŸš¨ ALERT: Redis system status issue detected!"
    echo "   - Predixy Status: $PREDIXY_STATUS"
    echo "   - Cluster Status: $CLUSTER_STATUS"
    echo "   - Timestamp: $TIMESTAMP"
    
    # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” Slack, ì´ë©”ì¼ ë“±ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡
    # curl -X POST -H 'Content-type: application/json' \
    #   --data "{\"text\":\"Redis Alert: $PREDIXY_STATUS / $CLUSTER_STATUS\"}" \
    #   $SLACK_WEBHOOK_URL
fi

echo "âœ… Monitoring check completed at $TIMESTAMP"
```

## 4. Spring Boot í†µí•© êµ¬í˜„

### 4.1 ì˜ì¡´ì„± ë° ê¸°ë³¸ ì„¤ì •

```gradle
// build.gradle
dependencies {
    // Redis ê´€ë ¨
    implementation 'org.springframework.boot:spring-boot-starter-data-redis'
    implementation project(':redis-service')
    
    // JSON ì²˜ë¦¬
    implementation 'com.fasterxml.jackson.core:jackson-databind'
    implementation 'com.fasterxml.jackson.datatype:jackson-datatype-jsr310'
    
    // ëª¨ë‹ˆí„°ë§
    implementation 'org.springframework.boot:spring-boot-starter-actuator'
    implementation 'io.micrometer:micrometer-registry-prometheus'
    
    // í…ŒìŠ¤íŠ¸
    testImplementation 'org.springframework.boot:spring-boot-starter-test'
    testImplementation 'org.testcontainers:junit-jupiter'
    testImplementation 'org.testcontainers:redis'
}
```

```yaml
# application.yml
spring:
  data:
    redis:
      host: 192.168.10.47  # Predixy í”„ë¡ì‹œ ì£¼ì†Œ
      port: 7617
      password: password123!
      timeout: 10s
      lettuce:
        pool:
          max-active: 20
          max-idle: 10
          min-idle: 5
          max-wait: 2s
        cluster:
          refresh:
            adaptive: true
            period: 30s

# ì»¤ìŠ¤í…€ ìºì‹œ ì„¤ì •
cache:
  redis:
    default-ttl: 3600
    key-prefix: "cache:"
    enable-monitoring: true
    serialization:
      strategy: "string"
      compression-threshold: 1024

# ë¡œê¹… ì„¤ì •
logging:
  level:
    com.company.redis: DEBUG
    org.springframework.data.redis: INFO
  pattern:
    console: '%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n'

# ëª¨ë‹ˆí„°ë§ ì„¤ì •
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  metrics:
    export:
      prometheus:
        enabled: true
    tags:
      application: ${spring.application.name}
      environment: ${ENVIRONMENT:dev}
```

### 4.2 Redis ì„¤ì • í´ë˜ìŠ¤

```java
/**
 * Redis ì„¤ì • í´ë˜ìŠ¤
 * - Predixy í”„ë¡ì‹œ ì—°ê²°
 * - StringRedisSerializer ìµœì í™”
 * - ì—°ê²° í’€ ì„¤ì •
 */
@Configuration
@EnableConfigurationProperties(CacheProperties.class)
@Slf4j
public class RedisConfig {
    
    @Value("${spring.data.redis.host}")
    private String redisHost;
    
    @Value("${spring.data.redis.port}")
    private int redisPort;
    
    @Value("${spring.data.redis.password}")
    private String redisPassword;
    
    @Bean
    public RedisConnectionFactory redisConnectionFactory() {
        // Lettuce ì—°ê²° íŒ©í† ë¦¬ ì„¤ì •
        RedisStandaloneConfiguration redisConfig = 
            new RedisStandaloneConfiguration(redisHost, redisPort);
        redisConfig.setPassword(redisPassword);
        
        // ì—°ê²° í’€ ìµœì í™”
        GenericObjectPoolConfig<StatefulRedisConnection<String, String>> poolConfig = 
            new GenericObjectPoolConfig<>();
        poolConfig.setMaxTotal(20);
        poolConfig.setMaxIdle(10);
        poolConfig.setMinIdle(5);
        poolConfig.setTestOnBorrow(true);
        poolConfig.setTestWhileIdle(true);
        poolConfig.setTimeBetweenEvictionRunsMillis(30000);
        
        LettucePoolingClientConfiguration clientConfig = LettucePoolingClientConfiguration.builder()
                .commandTimeout(Duration.ofSeconds(10))
                .shutdownTimeout(Duration.ofSeconds(5))
                .poolConfig(poolConfig)
                .build();
        
        LettuceConnectionFactory factory = new LettuceConnectionFactory(redisConfig, clientConfig);
        factory.afterPropertiesSet();
        
        log.info("Redis connection factory initialized: {}:{}", redisHost, redisPort);
        return factory;
    }
    
    @Bean(name = "redisTemplate")
    public RedisTemplate<String, Object> redisTemplate() {
        RedisTemplate<String, Object> redisTemplate = new RedisTemplate<>();
        
        redisTemplate.setConnectionFactory(redisConnectionFactory());
        
        // StringRedisSerializer ì‚¬ìš© - ìš©ëŸ‰ ìµœì í™”
        StringRedisSerializer stringSerializer = new StringRedisSerializer();
        redisTemplate.setKeySerializer(stringSerializer);
        redisTemplate.setValueSerializer(stringSerializer);
        
        // Hash ì§ë ¬í™” ì„¤ì •
        redisTemplate.setHashKeySerializer(stringSerializer);
        redisTemplate.setHashValueSerializer(new GenericJackson2JsonRedisSerializer());
        
        // ê¸°ë³¸ ì§ë ¬í™” ì„¤ì •
        redisTemplate.setDefaultSerializer(stringSerializer);
        
        // íŠ¸ëœì­ì…˜ ì§€ì› í™œì„±í™”
        redisTemplate.setEnableTransactionSupport(false);
        
        redisTemplate.afterPropertiesSet();
        
        log.info("RedisTemplate configured with StringRedisSerializer");
        return redisTemplate;
    }
    
    @Bean
    public RedisRepository redisRepository(RedisTemplate<String, Object> redisTemplate) {
        return new RedisRepository(redisTemplate);
    }
    
    @Bean
    public RedisProxy redisProxy(RedisRepository redisRepository, CacheProperties cacheProperties) {
        return new RedisProxy(redisRepository, cacheProperties);
    }
    
    @Bean
    public RedisDBProxy redisDBProxy(RedisRepository redisRepository, CacheProperties cacheProperties) {
        return new RedisDBProxy(redisRepository, cacheProperties);
    }
    
    @Bean
    public JsonSerializationUtil jsonSerializationUtil() {
        return new JsonSerializationUtil();
    }
}
```

## 5. DataFetcher íŒ¨í„´ ì‹¤ë¬´ êµ¬í˜„

### 5.1 DataFetcher ì¸í„°í˜ì´ìŠ¤ ë° êµ¬í˜„

```java
/**
 * DataFetcher ì¸í„°í˜ì´ìŠ¤
 * - ìºì‹œ ë¯¸ìŠ¤ ì‹œ DB ìë™ ì¡°íšŒ
 * - ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›ìœ¼ë¡œ N+1 ë¬¸ì œ í•´ê²°
 */
public interface DataFetcher {
    
    /**
     * ë‹¨ì¼ ë°ì´í„° ì¡°íšŒ
     */
    Object fetchData(String key);
    
    /**
     * ë°ì´í„° ì €ì¥ (ìºì‹œ ë¬´íš¨í™” ì‹œ ì‚¬ìš©)
     */
    void saveData(String key, Object value);
    
    /**
     * ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ - N+1 ë¬¸ì œ í•´ê²°
     */
    default Map<String, Object> fetchBatchData(List<String> keys) {
        return keys.stream()
                .parallel()
                .collect(Collectors.toMap(
                    key -> key,
                    this::fetchData,
                    (v1, v2) -> v1,
                    ConcurrentHashMap::new
                ));
    }
    
    /**
     * ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
     */
    default boolean exists(String key) {
        try {
            return fetchData(key) != null;
        } catch (Exception e) {
            return false;
        }
    }
}
```

### 5.2 Product ë„ë©”ì¸ DataFetcher êµ¬í˜„

```java
/**
 * Product ë„ë©”ì¸ DataFetcher êµ¬í˜„
 * - JPA Repositoryì™€ ì—°ë™
 * - ë°°ì¹˜ ì¡°íšŒ ìµœì í™”
 * - ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨
 */
@Component
@Slf4j
public class ProductDataFetcher implements DataFetcher {
    
    private final ProductRepository productRepository;
    private final JsonSerializationUtil jsonUtil;
    private final MeterRegistry meterRegistry;
    
    private final Counter dbFetchCounter;
    private final Timer dbFetchTimer;
    
    public ProductDataFetcher(ProductRepository productRepository,
                             JsonSerializationUtil jsonUtil,
                             MeterRegistry meterRegistry) {
        this.productRepository = productRepository;
        this.jsonUtil = jsonUtil;
        this.meterRegistry = meterRegistry;
        
        // ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        this.dbFetchCounter = Counter.builder("cache.db.fetch")
                .tag("domain", "product")
                .register(meterRegistry);
        this.dbFetchTimer = Timer.builder("cache.db.fetch.duration")
                .tag("domain", "product")
                .register(meterRegistry);
    }
    
    @Override
    public Object fetchData(String key) {
        return dbFetchTimer.recordCallable(() -> {
            try {
                String productId = extractProductId(key);
                
                log.debug("Fetching product from DB: {}", productId);
                
                Product product = productRepository.findById(productId)
                        .orElseThrow(() -> new EntityNotFoundException("Product not found: " + productId));
                
                // DTO ë³€í™˜
                ProductDTO productDTO = convertToDTO(product);
                
                // JSON ì§ë ¬í™”
                String jsonData = jsonUtil.serialize(productDTO);
                
                dbFetchCounter.increment();
                log.info("Product fetched from DB and serialized: {}", productId);
                
                return jsonData;
                
            } catch (EntityNotFoundException e) {
                log.warn("Product not found for key: {}", key);
                return null;
            } catch (Exception e) {
                log.error("Failed to fetch product data for key: {}", key, e);
                throw new DataFetchException("DB ì¡°íšŒ ì‹¤íŒ¨: " + key, e);
            }
        });
    }
    
    @Override
    public void saveData(String key, Object value) {
        try {
            String productId = extractProductId(key);
            
            // JSON ì—­ì§ë ¬í™”
            ProductDTO productDTO = jsonUtil.deserialize(value.toString(), ProductDTO.class);
            
            // Entity ë³€í™˜
            Product product = convertToEntity(productDTO);
            product.setUpdatedAt(LocalDateTime.now());
            
            productRepository.save(product);
            
            log.info("Product saved to DB: {}", productId);
            
        } catch (Exception e) {
            log.error("Failed to save product data for key: {}", key, e);
            throw new DataFetchException("DB ì €ì¥ ì‹¤íŒ¨: " + key, e);
        }
    }
    
    @Override
    public Map<String, Object> fetchBatchData(List<String> keys) {
        return dbFetchTimer.recordCallable(() -> {
            try {
                // í‚¤ì—ì„œ Product ID ëª©ë¡ ì¶”ì¶œ
                List<String> productIds = keys.stream()
                        .map(this::extractProductId)
                        .collect(Collectors.toList());
                
                log.debug("Batch fetching products from DB: {}", productIds);
                
                // ë°°ì¹˜ ì¡°íšŒë¡œ N+1 ë¬¸ì œ í•´ê²°
                List<Product> products = productRepository.findAllById(productIds);
                
                Map<String, Object> result = new ConcurrentHashMap<>();
                for (Product product : products) {
                    String key = "product:" + product.getProductId();
                    ProductDTO productDTO = convertToDTO(product);
                    String jsonData = jsonUtil.serialize(productDTO);
                    result.put(key, jsonData);
                }
                
                dbFetchCounter.increment(products.size());
                log.info("Batch fetched {} products from DB", products.size());
                
                return result;
                
            } catch (Exception e) {
                log.error("Failed to batch fetch product data for keys: {}", keys, e);
                throw new DataFetchException("ë°°ì¹˜ DB ì¡°íšŒ ì‹¤íŒ¨", e);
            }
        });
    }
    
    private String extractProductId(String key) {
        // "product:P001" -> "P001"
        if (!key.startsWith("product:")) {
            throw new IllegalArgumentException("Invalid product key format: " + key);
        }
        return key.substring(8); // "product:" ê¸¸ì´ë§Œí¼ ì œê±°
    }
    
    private ProductDTO convertToDTO(Product product) {
        return ProductDTO.builder()
                .productId(product.getProductId())
                .name(product.getName())
                .description(product.getDescription())
                .price(product.getPrice())
                .category(product.getCategory())
                .stockCount(product.getStockCount())
                .status(product.getStatus())
                .tags(product.getTags())
                .createdAt(product.getCreatedAt())
                .updatedAt(product.getUpdatedAt())
                .build();
    }
    
    private Product convertToEntity(ProductDTO dto) {
        Product product = new Product();
        product.setProductId(dto.getProductId());
        product.setName(dto.getName());
        product.setDescription(dto.getDescription());
        product.setPrice(dto.getPrice());
        product.setCategory(dto.getCategory());
        product.setStockCount(dto.getStockCount());
        product.setStatus(dto.getStatus());
        product.setTags(dto.getTags());
        product.setCreatedAt(dto.getCreatedAt());
        product.setUpdatedAt(dto.getUpdatedAt());
        return product;
    }
}
```

### 5.3 RedisDBProxy êµ¬í˜„

```java
/**
 * RedisDBProxy - DB ì—°ë™ ìºì‹œ í”„ë¡ì‹œ
 * - DataFetcher íŒ¨í„´ í™œìš©
 * - Look-aside ìºì‹œ ì „ëµ êµ¬í˜„
 * - ìë™ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
 */
@Component
@Slf4j
public class RedisDBProxy {
    
    private final RedisRepository redisRepository;
    private final CacheProperties cacheProperties;
    private final MeterRegistry meterRegistry;
    
    private DataFetcher dataFetcher;
    
    // ë©”íŠ¸ë¦­ ìˆ˜ì§‘ìš©
    private final Counter cacheHitCounter;
    private final Counter cacheMissCounter;
    private final Counter dbFetchCounter;
    private final Counter cacheErrorCounter;
    private final Timer cacheOperationTimer;
    
    public RedisDBProxy(RedisRepository redisRepository,
                       CacheProperties cacheProperties,
                       MeterRegistry meterRegistry) {
        this.redisRepository = redisRepository;
        this.cacheProperties = cacheProperties;
        this.meterRegistry = meterRegistry;
        
        // ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        this.cacheHitCounter = Counter.builder("cache.hit")
                .tag("proxy", "db")
                .register(meterRegistry);
        this.cacheMissCounter = Counter.builder("cache.miss")
                .tag("proxy", "db")
                .register(meterRegistry);
        this.dbFetchCounter = Counter.builder("cache.db.fetch")
                .tag("proxy", "db")
                .register(meterRegistry);
        this.cacheErrorCounter = Counter.builder("cache.error")
                .tag("proxy", "db")
                .register(meterRegistry);
        this.cacheOperationTimer = Timer.builder("cache.operation.duration")
                .tag("proxy", "db")
                .register(meterRegistry);
    }
    
    /**
     * DataFetcher ì—°ê²°
     */
    public void connectFetcher(DataFetcher dataFetcher) {
        this.dataFetcher = dataFetcher;
        log.info("DataFetcher connected: {}", dataFetcher.getClass().getSimpleName());
    }
    
    /**
     * DataFetcher ì—°ê²° í•´ì œ
     */
    public void disconnectFetcher() {
        this.dataFetcher = null;
        log.info("DataFetcher disconnected");
    }
    
    /**
     * DataFetcher ì—°ê²° ìƒíƒœ í™•ì¸
     */
    public boolean ifDataFetcher() {
        return this.dataFetcher != null;
    }
    
    /**
     * ìºì‹œì—ì„œ ê°’ ì¡°íšŒ (ìºì‹œ ë¯¸ìŠ¤ ì‹œ DB ìë™ ì¡°íšŒ)
     */
    public Object get(String key) {
        return cacheOperationTimer.recordCallable(() -> {
            try {
                String fullKey = buildKey(key);
                
                // 1. ìºì‹œì—ì„œ ì¡°íšŒ
                Object cachedValue = redisRepository.get(fullKey);
                if (cachedValue != null) {
                    cacheHitCounter.increment();
                    log.debug("Cache hit for key: {}", key);
                    return cachedValue;
                }
                
                // 2. ìºì‹œ ë¯¸ìŠ¤
                cacheMissCounter.increment();
                log.debug("Cache miss for key: {}", key);
                
                if (dataFetcher == null) {
                    log.warn("DataFetcher not connected for key: {}", key);
                    return null;
                }
                
                // 3. DBì—ì„œ ë°ì´í„° ì¡°íšŒ
                Object dbValue = dataFetcher.fetchData(key);
                
                if (dbValue != null) {
                    // 4. ì¡°íšŒëœ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥
                    redisRepository.set(fullKey, dbValue, 
                        cacheProperties.getDefaultTtl(), TimeUnit.SECONDS);
                    
                    dbFetchCounter.increment();
                    log.info("Data loaded from DB and cached for key: {}", key);
                    return dbValue;
                }
                
                return null;
                
            } catch (Exception e) {
                cacheErrorCounter.increment();
                log.error("Cache get with DB fetch error for key: {}", key, e);
                return null;
            }
        });
    }
    
    /**
     * ìºì‹œì— ê°’ ì €ì¥
     */
    public void set(String key, Object value) {
        set(key, value, cacheProperties.getDefaultTtl(), TimeUnit.SECONDS);
    }
    
    /**
     * ìºì‹œì— ê°’ ì €ì¥ (TTL ì§€ì •)
     */
    public void set(String key, Object value, long ttl, TimeUnit timeUnit) {
        try {
            String fullKey = buildKey(key);
            
            // ìºì‹œì— ì €ì¥
            redisRepository.set(fullKey, value, ttl, timeUnit);
            
            log.debug("Cache set for key: {} with TTL: {} {}", key, ttl, timeUnit);
            
        } catch (Exception e) {
            cacheErrorCounter.increment();
            log.error("Cache set error for key: {}", key, e);
        }
    }
    
    /**
     * ë°ì´í„° ê°•ì œ ë¡œë“œ (DB â†’ ìºì‹œ)
     */
    public void load(String key) {
        try {
            if (dataFetcher == null) {
                log.warn("Cannot load data - DataFetcher not connected for key: {}", key);
                return;
            }
            
            Object dbValue = dataFetcher.fetchData(key);
            
            if (dbValue != null) {
                String fullKey = buildKey(key);
                redisRepository.set(fullKey, dbValue, 
                    cacheProperties.getDefaultTtl(), TimeUnit.SECONDS);
                
                dbFetchCounter.increment();
                log.info("Data forcefully loaded from DB to cache for key: {}", key);
            } else {
                log.warn("No data found in DB for key: {}", key);
            }
            
        } catch (Exception e) {
            cacheErrorCounter.increment();
            log.error("Data load error for key: {}", key, e);
        }
    }
    
    /**
     * ë§Œë£Œ ì‹œê°„ ì„¤ì •
     */
    public boolean setExpire(String key, long ttl, TimeUnit timeUnit) {
        try {
            String fullKey = buildKey(key);
            return redisRepository.setExpire(fullKey, ttl, timeUnit);
        } catch (Exception e) {
            cacheErrorCounter.increment();
            log.error("Cache setExpire error for key: {}", key, e);
            return false;
        }
    }
    
    /**
     * ë§Œë£Œ ì‹œê°„ ì¡°íšŒ
     */
    public Long getExpire(String key, TimeUnit timeUnit) {
        try {
            String fullKey = buildKey(key);
            return redisRepository.getExpire(fullKey, timeUnit);
        } catch (Exception e) {
            cacheErrorCounter.increment();
            log.error("Cache getExpire error for key: {}", key, e);
            return -1L;
        }
    }
    
    /**
     * ìºì‹œ ì‚­ì œ
     */
    public boolean delete(String key) {
        try {
            String fullKey = buildKey(key);
            return redisRepository.delete(fullKey);
        } catch (Exception e) {
            cacheErrorCounter.increment();
            log.error("Cache delete error for key: {}", key, e);
            return false;
        }
    }
    
    /**
     * ë°°ì¹˜ ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)
     */
    public Map<String, Object> getBatch(List<String> keys) {
        if (keys == null || keys.isEmpty()) {
            return Collections.emptyMap();
        }
        
        try {
            // ìºì‹œ í‚¤ ë³€í™˜
            List<String> fullKeys = keys.stream()
                    .map(this::buildKey)
                    .collect(Collectors.toList());
            
            // ë°°ì¹˜ ìºì‹œ ì¡°íšŒ
            List<Object> cachedValues = redisRepository.multiGet(fullKeys);
            
            Map<String, Object> result = new HashMap<>();
            List<String> missedKeys = new ArrayList<>();
            
            // ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ ë¶„ë¥˜
            for (int i = 0; i < keys.size(); i++) {
                String originalKey = keys.get(i);
                Object cachedValue = cachedValues.get(i);
                
                if (cachedValue != null) {
                    result.put(originalKey, cachedValue);
                    cacheHitCounter.increment();
                } else {
                    missedKeys.add(originalKey);
                    cacheMissCounter.increment();
                }
            }
            
            // ìºì‹œ ë¯¸ìŠ¤ ë°ì´í„°ë¥¼ DBì—ì„œ ë°°ì¹˜ ì¡°íšŒ
            if (!missedKeys.isEmpty() && dataFetcher != null) {
                Map<String, Object> dbValues = dataFetcher.fetchBatchData(missedKeys);
                
                // DB ì¡°íšŒ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ê³  ê²°ê³¼ì— ì¶”ê°€
                if (!dbValues.isEmpty()) {
                    Map<String, Object> cacheData = new HashMap<>();
                    for (Map.Entry<String, Object> entry : dbValues.entrySet()) {
                        String key = entry.getKey();
                        Object value = entry.getValue();
                        
                        result.put(key, value);
                        cacheData.put(buildKey(key), value);
                    }
                    
                    // ë°°ì¹˜ ìºì‹œ ì €ì¥
                    redisRepository.multiSet(cacheData);
                    dbFetchCounter.increment(dbValues.size());
                }
            }
            
            log.debug("Batch get completed - Total: {}, Hits: {}, Misses: {}", 
                    keys.size(), result.size() - missedKeys.size(), missedKeys.size());
            
            return result;
            
        } catch (Exception e) {
            cacheErrorCounter.increment();
            log.error("Batch cache get error for keys: {}", keys, e);
            return Collections.emptyMap();
        }
    }
    
    private String buildKey(String key) {
        return cacheProperties.getKeyPrefix() + key;
    }
}
```

## 6. ì„œë¹„ìŠ¤ ë ˆì´ì–´ í†µí•©

### 6.1 ProductService ì‹¤ë¬´ êµ¬í˜„

```java
/**
 * ProductService - RedisDBProxy í™œìš©
 * - ìºì‹œ ìš°ì„  ì¡°íšŒ
 * - ìë™ DB ì—°ë™
 * - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì§‘ì¤‘
 */
@Service
@Slf4j
@Transactional(readOnly = true)
public class ProductService {
    
    private final RedisDBProxy redisDBProxy;
    private final ProductDataFetcher productDataFetcher;
    private final JsonSerializationUtil jsonUtil;
    private final ProductRepository productRepository;
    
    public ProductService(RedisDBProxy redisDBProxy,
                         ProductDataFetcher productDataFetcher,
                         JsonSerializationUtil jsonUtil,
                         ProductRepository productRepository) {
        this.redisDBProxy = redisDBProxy;
        this.productDataFetcher = productDataFetcher;
        this.jsonUtil = jsonUtil;
        this.productRepository = productRepository;
        
        // DataFetcher ì—°ê²°
        this.redisDBProxy.connectFetcher(productDataFetcher);
    }
    
    /**
     * ìƒí’ˆ ì¡°íšŒ - ìºì‹œ ìš°ì„ , ë¯¸ìŠ¤ ì‹œ DB ìë™ ì¡°íšŒ
     */
    public ProductDTO getProduct(String productId) {
        try {
            String key = "product:" + productId;
            Object cachedData = redisDBProxy.get(key);
            
            if (cachedData != null) {
                return jsonUtil.deserialize(cachedData.toString(), ProductDTO.class);
            }
            
            // ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ DBì—ë„ ë°ì´í„°ê°€ ì—†ìŒ
            throw new EntityNotFoundException("Product not found: " + productId);
            
        } catch (EntityNotFoundException e) {
            throw e;
        } catch (Exception e) {
            log.error("Failed to get product: {}", productId, e);
            throw new ServiceException("ìƒí’ˆ ì¡°íšŒ ì‹¤íŒ¨", e);
        }
    }
    
    /**
     * ìƒí’ˆ ëª©ë¡ ì¡°íšŒ - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
     */
    public List<ProductDTO> getProducts(List<String> productIds) {
        try {
            List<String> keys = productIds.stream()
                    .map(id -> "product:" + id)
                    .collect(Collectors.toList());
            
            Map<String, Object> cachedData = redisDBProxy.getBatch(keys);
            
            List<ProductDTO> products = new ArrayList<>();
            for (String productId : productIds) {
                String key = "product:" + productId;
                Object data = cachedData.get(key);
                
                if (data != null) {
                    ProductDTO product = jsonUtil.deserialize(data.toString(), ProductDTO.class);
                    products.add(product);
                }
            }
            
            return products;
            
        } catch (Exception e) {
            log.error("Failed to get products: {}", productIds, e);
            throw new ServiceException("ìƒí’ˆ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨", e);
        }
    }
    
    /**
     * ìƒí’ˆ ìƒì„± - Write-Through ìºì‹œ
     */
    @Transactional
    public ProductDTO createProduct(CreateProductRequest request) {
        try {
            // 1. DBì— ì €ì¥
            Product product = new Product();
            product.setProductId(generateProductId());
            product.setName(request.getName());
            product.setDescription(request.getDescription());
            product.setPrice(request.getPrice());
            product.setCategory(request.getCategory());
            product.setStockCount(request.getStockCount());
            product.setStatus(ProductStatus.ACTIVE);
            product.setTags(request.getTags());
            product.setCreatedAt(LocalDateTime.now());
            product.setUpdatedAt(LocalDateTime.now());
            
            Product savedProduct = productRepository.save(product);
            
            // 2. ìºì‹œì— ì €ì¥
            ProductDTO productDTO = convertToDTO(savedProduct);
            String key = "product:" + productDTO.getProductId();
            String jsonData = jsonUtil.serialize(productDTO);
            
            redisDBProxy.set(key, jsonData, 3600, TimeUnit.SECONDS); // 1ì‹œê°„ TTL
            
            log.info("Product created and cached: {}", productDTO.getProductId());
            return productDTO;
            
        } catch (Exception e) {
            log.error("Failed to create product: {}", request.getName(), e);
            throw new ServiceException("ìƒí’ˆ ìƒì„± ì‹¤íŒ¨", e);
        }
    }
    
    /**
     * ìƒí’ˆ ì—…ë°ì´íŠ¸ - Write-Through ìºì‹œ
     */
    @Transactional
    public ProductDTO updateProduct(String productId, UpdateProductRequest request) {
        try {
            // 1. DB ì—…ë°ì´íŠ¸
            Product product = productRepository.findById(productId)
                    .orElseThrow(() -> new EntityNotFoundException("Product not found: " + productId));
            
            updateProductFields(product, request);
            product.setUpdatedAt(LocalDateTime.now());
            
            Product savedProduct = productRepository.save(product);
            
            // 2. ìºì‹œ ì—…ë°ì´íŠ¸
            ProductDTO productDTO = convertToDTO(savedProduct);
            String key = "product:" + productId;
            String jsonData = jsonUtil.serialize(productDTO);
            
            redisDBProxy.set(key, jsonData, 3600, TimeUnit.SECONDS);
            
            log.info("Product updated and cache refreshed: {}", productId);
            return productDTO;
            
        } catch (EntityNotFoundException e) {
            throw e;
        } catch (Exception e) {
            log.error("Failed to update product: {}", productId, e);
            throw new ServiceException("ìƒí’ˆ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨", e);
        }
    }
    
    /**
     * ìƒí’ˆ ì‚­ì œ - ìºì‹œ ë¬´íš¨í™”
     */
    @Transactional
    public void deleteProduct(String productId) {
        try {
            // 1. DBì—ì„œ ì‚­ì œ
            if (!productRepository.existsById(productId)) {
                throw new EntityNotFoundException("Product not found: " + productId);
            }
            
            productRepository.deleteById(productId);
            
            // 2. ìºì‹œ ë¬´íš¨í™”
            String key = "product:" + productId;
            redisDBProxy.delete(key);
            
            log.info("Product deleted and cache invalidated: {}", productId);
            
        } catch (EntityNotFoundException e) {
            throw e;
        } catch (Exception e) {
            log.error("Failed to delete product: {}", productId, e);
            throw new ServiceException("ìƒí’ˆ ì‚­ì œ ì‹¤íŒ¨", e);
        }
    }
    
    /**
     * ìƒí’ˆ ìºì‹œ ê°•ì œ ìƒˆë¡œê³ ì¹¨
     */
    public void refreshProductCache(String productId) {
        try {
            String key = "product:" + productId;
            
            // ê¸°ì¡´ ìºì‹œ ì‚­ì œ í›„ DBì—ì„œ ê°•ì œ ë¡œë“œ
            redisDBProxy.delete(key);
            redisDBProxy.load(key);
            
            log.info("Product cache refreshed: {}", productId);
            
        } catch (Exception e) {
            log.error("Failed to refresh product cache: {}", productId, e);
            throw new ServiceException("ìƒí’ˆ ìºì‹œ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨", e);
        }
    }
    
    /**
     * ìƒí’ˆ ì¬ê³  ì—…ë°ì´íŠ¸ - ìì£¼ ë³€ê²½ë˜ëŠ” ë°ì´í„°ì˜ ìºì‹œ ì „ëµ
     */
    @Transactional
    public void updateProductStock(String productId, int newStockCount) {
        try {
            // 1. DB ì—…ë°ì´íŠ¸
            int updated = productRepository.updateStockCount(productId, newStockCount);
            if (updated == 0) {
                throw new EntityNotFoundException("Product not found: " + productId);
            }
            
            // 2. ìºì‹œ ë¬´íš¨í™” (ì¬ê³ ëŠ” ìì£¼ ë³€ê²½ë˜ë¯€ë¡œ ì¦‰ì‹œ ì‚­ì œ)
            String key = "product:" + productId;
            redisDBProxy.delete(key);
            
            log.info("Product stock updated and cache invalidated: {} -> {}", productId, newStockCount);
            
        } catch (EntityNotFoundException e) {
            throw e;
        } catch (Exception e) {
            log.error("Failed to update product stock: {}", productId, e);
            throw new ServiceException("ìƒí’ˆ ì¬ê³  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨", e);
        }
    }
    
    private String generateProductId() {
        return "P" + System.currentTimeMillis() + String.format("%03d", (int)(Math.random() * 1000));
    }
    
    private ProductDTO convertToDTO(Product product) {
        return ProductDTO.builder()
                .productId(product.getProductId())
                .name(product.getName())
                .description(product.getDescription())
                .price(product.getPrice())
                .category(product.getCategory())
                .stockCount(product.getStockCount())
                .status(product.getStatus())
                .tags(product.getTags())
                .createdAt(product.getCreatedAt())
                .updatedAt(product.getUpdatedAt())
                .build();
    }
    
    private void updateProductFields(Product product, UpdateProductRequest request) {
        if (request.getName() != null) {
            product.setName(request.getName());
        }
        if (request.getDescription() != null) {
            product.setDescription(request.getDescription());
        }
        if (request.getPrice() != null) {
            product.setPrice(request.getPrice());
        }
        if (request.getCategory() != null) {
            product.setCategory(request.getCategory());
        }
        if (request.getStockCount() != null) {
            product.setStockCount(request.getStockCount());
        }
        if (request.getStatus() != null) {
            product.setStatus(request.getStatus());
        }
        if (request.getTags() != null) {
            product.setTags(request.getTags());
        }
    }
}
```

## 7. ìš´ì˜ ì´ìŠˆ ë° í•´ê²°

### 7.1 Redis í´ëŸ¬ìŠ¤í„° ê¶Œí•œ ë¬¸ì œ

```bash
# ë¬¸ì œ ìƒí™©: slot redirect ì‹œ ê¶Œí•œ ì—ëŸ¬ ë°œìƒ
# (error) NOAUTH Authentication required.

# ì›ì¸ ë¶„ì„
# Redis í´ëŸ¬ìŠ¤í„°ì—ì„œ MOVED/ASK redirect ë°œìƒ ì‹œ 
# ìƒˆë¡œìš´ ë…¸ë“œì—ì„œ ì¸ì¦ ì •ë³´ê°€ ëˆ„ë½ë¨

# í•´ê²° ë°©ë²• 1: redis-cli ì ‘ì† ì‹œ ê¶Œí•œ ì •ë³´ í¬í•¨
redis-cli -c -p 6300 --user default --pass 'password123!'

# í•´ê²° ë°©ë²• 2: ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ connection factory ì„¤ì •
# RedisConfig.javaì—ì„œ password ì„¤ì • í™•ì¸
redisConfig.setPassword(redisPassword);

# í•´ê²° ë°©ë²• 3: Predixy í”„ë¡ì‹œ ì‚¬ìš©ìœ¼ë¡œ ë¬¸ì œ íšŒí”¼
# í´ë¼ì´ì–¸íŠ¸ëŠ” Predixyë§Œ ì ‘ê·¼í•˜ë¯€ë¡œ redirect ì´ìŠˆ ì—†ìŒ
redis-cli -h 192.168.10.47 -p 7617 ping

# ë¬¸ì œ í•´ê²° í™•ì¸
echo "Testing Redis cluster through Predixy..."
for i in {1..10}; do
    redis-cli -h 192.168.10.47 -p 7617 set "test:$i" "value$i"
    redis-cli -h 192.168.10.47 -p 7617 get "test:$i"
done
echo "All tests passed!"
```

### 7.2 ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°

```bash
#!/bin/bash
# redis-memory-monitor.sh - Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

REDIS_HOST="192.168.10.47"
REDIS_PORT="7617"
ALERT_THRESHOLD=80  # 80% ì„ê³„ê°’

while true; do
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸
    MEMORY_INFO=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT info memory 2>/dev/null)
    
    if [ $? -eq 0 ]; then
        USED_MEMORY=$(echo "$MEMORY_INFO" | grep "used_memory:" | cut -d: -f2 | tr -d '\r')
        MAX_MEMORY=$(echo "$MEMORY_INFO" | grep "maxmemory:" | cut -d: -f2 | tr -d '\r')
        
        if [ "$MAX_MEMORY" -gt 0 ]; then
            USAGE_PERCENT=$((USED_MEMORY * 100 / MAX_MEMORY))
            
            echo "$(date): Redis Memory Usage: $USAGE_PERCENT% ($USED_MEMORY / $MAX_MEMORY bytes)"
            
            if [ "$USAGE_PERCENT" -gt "$ALERT_THRESHOLD" ]; then
                echo "ğŸš¨ ALERT: Redis memory usage above threshold!"
                echo "   Current usage: $USAGE_PERCENT%"
                echo "   Used memory: $USED_MEMORY bytes"
                echo "   Max memory: $MAX_MEMORY bytes"
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì„ ë•Œ í‚¤ ë¶„ì„
                echo "ğŸ“Š Top memory consuming keys:"
                redis-cli -h $REDIS_HOST -p $REDIS_PORT --bigkeys
                
                # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ì•Œë¦¼ ë°œì†¡
                # send_alert "Redis memory usage: $USAGE_PERCENT%"
            fi
        fi
    else
        echo "$(date): Failed to connect to Redis"
    fi
    
    sleep 60  # 1ë¶„ë§ˆë‹¤ í™•ì¸
done
```

### 7.3 ì„±ëŠ¥ ìµœì í™” íŠœë‹

```java
/**
 * Redis ì„±ëŠ¥ ìµœì í™” ì„¤ì •
 */
@Component
@Slf4j
public class RedisPerformanceTuner {
    
    private final RedisTemplate<String, Object> redisTemplate;
    private final CacheProperties cacheProperties;
    
    public RedisPerformanceTuner(RedisTemplate<String, Object> redisTemplate,
                                CacheProperties cacheProperties) {
        this.redisTemplate = redisTemplate;
        this.cacheProperties = cacheProperties;
    }
    
    /**
     * Redis ì„¤ì • ìµœì í™”
     */
    @PostConstruct
    public void optimizeRedisSettings() {
        try {
            // ë©”ëª¨ë¦¬ ì •ì±… í™•ì¸ ë° ìµœì í™”
            String maxMemoryPolicy = redisTemplate.execute((RedisCallback<String>) connection -> {
                return connection.getConfig("maxmemory-policy").get("maxmemory-policy");
            });
            
            if (!"allkeys-lru".equals(maxMemoryPolicy)) {
                log.warn("Redis maxmemory-policy is not optimal: {}", maxMemoryPolicy);
                // ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ìë™ ë³€ê²½í•˜ì§€ ì•Šê³  ì•Œë¦¼ë§Œ
                sendOptimizationAlert("Redis maxmemory-policy ìµœì í™” í•„ìš”: " + maxMemoryPolicy);
            }
            
            // ì—°ê²° ìƒíƒœ í™•ì¸
            String ping = redisTemplate.execute((RedisCallback<String>) connection -> {
                return connection.ping();
            });
            
            if ("PONG".equals(ping)) {
                log.info("Redis connection optimization check completed successfully");
                analyzeKeyDistribution();
            }
            
        } catch (Exception e) {
            log.error("Failed to optimize Redis settings", e);
        }
    }
    
    /**
     * í‚¤ ë¶„í¬ ë¶„ì„
     */
    private void analyzeKeyDistribution() {
        try {
            // í‚¤ íŒ¨í„´ë³„ ë¶„ì„
            Map<String, Long> keyPatternCounts = new HashMap<>();
            
            // ìƒ˜í”Œë§ì„ í†µí•œ í‚¤ ë¶„ì„ (ì „ì²´ í‚¤ë¥¼ ë‹¤ ë³´ë©´ ì„±ëŠ¥ ì˜í–¥)
            Set<String> sampleKeys = redisTemplate.execute((RedisCallback<Set<String>>) connection -> {
                return connection.keys(cacheProperties.getKeyPrefix() + "*");
            });
            
            if (sampleKeys != null) {
                for (String key : sampleKeys) {
                    String pattern = extractKeyPattern(key);
                    keyPatternCounts.merge(pattern, 1L, Long::sum);
                }
                
                log.info("Key distribution analysis:");
                keyPatternCounts.entrySet().stream()
                        .sorted(Map.Entry.<String, Long>comparingByValue().reversed())
                        .limit(10)
                        .forEach(entry -> 
                            log.info("  {}: {} keys", entry.getKey(), entry.getValue()));
            }
            
        } catch (Exception e) {
            log.error("Failed to analyze key distribution", e);
        }
    }
    
    /**
     * ìºì‹œ íˆíŠ¸ìœ¨ ë¶„ì„ ë° ìµœì í™” ì œì•ˆ
     */
    @Scheduled(fixedRate = 300000) // 5ë¶„ë§ˆë‹¤
    public void analyzeCachePerformance() {
        try {
            String info = redisTemplate.execute((RedisCallback<String>) connection -> {
                return connection.info("stats");
            });
            
            if (info != null) {
                Map<String, String> stats = parseRedisInfo(info);
                
                long keyspaceHits = Long.parseLong(stats.getOrDefault("keyspace_hits", "0"));
                long keyspaceMisses = Long.parseLong(stats.getOrDefault("keyspace_misses", "0"));
                
                if (keyspaceHits + keyspaceMisses > 0) {
                    double hitRatio = (double) keyspaceHits / (keyspaceHits + keyspaceMisses) * 100;
                    
                    log.info("Redis hit ratio: {:.2f}% (hits: {}, misses: {})", 
                            hitRatio, keyspaceHits, keyspaceMisses);
                    
                    if (hitRatio < 70.0) {
                        sendOptimizationAlert(String.format(
                            "Redis íˆíŠ¸ìœ¨ ë‚®ìŒ: %.2f%% - TTL ì •ì±… ê²€í†  í•„ìš”", hitRatio));
                    }
                }
            }
            
        } catch (Exception e) {
            log.error("Failed to analyze cache performance", e);
        }
    }
    
    private String extractKeyPattern(String key) {
        // "cache:product:P001" -> "product"
        String[] parts = key.split(":");
        return parts.length > 2 ? parts[1] : "unknown";
    }
    
    private Map<String, String> parseRedisInfo(String info) {
        Map<String, String> result = new HashMap<>();
        String[] lines = info.split("\r\n");
        
        for (String line : lines) {
            if (line.contains(":") && !line.startsWith("#")) {
                String[] parts = line.split(":", 2);
                if (parts.length == 2) {
                    result.put(parts[0], parts[1]);
                }
            }
        }
        
        return result;
    }
    
    private void sendOptimizationAlert(String message) {
        // ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” Slack, ì´ë©”ì¼ ë“±ìœ¼ë¡œ ì•Œë¦¼
        log.warn("OPTIMIZATION ALERT: {}", message);
    }
}
```

## 8. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 8.1 í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```java
/**
 * Redis í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
 * - ì‹¤ì œ ìš´ì˜ í™˜ê²½ê³¼ ìœ ì‚¬í•œ ì¡°ê±´ì—ì„œ í…ŒìŠ¤íŠ¸
 * - ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ì¸¡ì •
 */
@SpringBootTest
@Slf4j
class RedisClusterPerformanceTest {
    
    @Autowired
    private RedisDBProxy redisDBProxy;
    
    @Autowired
    private ProductDataFetcher productDataFetcher;
    
    @Autowired
    private MeterRegistry meterRegistry;
    
    @Autowired
    private JsonSerializationUtil jsonUtil;
    
    @BeforeEach
    void setUp() {
        redisDBProxy.connectFetcher(productDataFetcher);
    }
    
    @Test
    @DisplayName("ë‹¨ì¼ ìºì‹œ ì¡°íšŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    void testSingleCachePerformance() {
        // Given
        String key = "product:PERF001";
        int iterations = 1000;
        
        // ì‚¬ì „ì— ë°ì´í„° ìºì‹±
        ProductDTO product = createTestProduct("PERF001");
        String jsonData = jsonUtil.serialize(product);
        redisDBProxy.set(key, jsonData);
        
        // When
        long startTime = System.currentTimeMillis();
        
        for (int i = 0; i < iterations; i++) {
            Object result = redisDBProxy.get(key);
            assertThat(result).isNotNull();
        }
        
        long endTime = System.currentTimeMillis();
        long totalTime = endTime - startTime;
        
        // Then
        double avgResponseTime = (double) totalTime / iterations;
        log.info("Single cache performance test results:");
        log.info("  - Iterations: {}", iterations);
        log.info("  - Total time: {} ms", totalTime);
        log.info("  - Average response time: {:.2f} ms/request", avgResponseTime);
        log.info("  - Throughput: {:.2f} requests/second", 1000.0 / avgResponseTime);
        
        // ì„±ëŠ¥ ê¸°ì¤€: í‰ê·  ì‘ë‹µì‹œê°„ 5ms ì´í•˜ (ì‹ ë¢°ë„: 90%)
        assertThat(avgResponseTime).isLessThan(5.0);
    }
    
    @Test
    @DisplayName("ë™ì‹œ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    void testConcurrentPerformance() throws InterruptedException {
        // Given
        int threadCount = 50;
        int requestsPerThread = 100;
        String keyPrefix = "product:CONCURRENT";
        
        // í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ì „ ìºì‹±
        for (int i = 0; i < threadCount * requestsPerThread; i++) {
            String key = keyPrefix + i;
            ProductDTO product = createTestProduct("CONCURRENT" + i);
            String jsonData = jsonUtil.serialize(product);
            redisDBProxy.set(key, jsonData);
        }
        
        ExecutorService executorService = Executors.newFixedThreadPool(threadCount);
        CountDownLatch latch = new CountDownLatch(threadCount);
        AtomicInteger successCount = new AtomicInteger(0);
        AtomicLong totalResponseTime = new AtomicLong(0);
        
        // When
        long startTime = System.currentTimeMillis();
        
        for (int i = 0; i < threadCount; i++) {
            final int threadId = i;
            executorService.submit(() -> {
                try {
                    for (int j = 0; j < requestsPerThread; j++) {
                        long requestStart = System.nanoTime();
                        
                        String key = keyPrefix + (threadId * requestsPerThread + j);
                        Object result = redisDBProxy.get(key);
                        
                        long requestEnd = System.nanoTime();
                        totalResponseTime.addAndGet(requestEnd - requestStart);
                        
                        if (result != null) {
                            successCount.incrementAndGet();
                        }
                    }
                } finally {
                    latch.countDown();
                }
            });
        }
        
        latch.await(30, TimeUnit.SECONDS);
        executorService.shutdown();
        
        long endTime = System.currentTimeMillis();
        
        // Then
        int totalRequests = threadCount * requestsPerThread;
        double avgResponseTimeMs = totalResponseTime.get() / 1_000_000.0 / totalRequests;
        double throughput = (double) totalRequests / (endTime - startTime) * 1000;
        double successRate = (double) successCount.get() / totalRequests * 100;
        
        log.info("Concurrent performance test results:");
        log.info("  - Threads: {}", threadCount);
        log.info("  - Requests per thread: {}", requestsPerThread);
        log.info("  - Total requests: {}", totalRequests);
        log.info("  - Success rate: {:.2f}%", successRate);
        log.info("  - Average response time: {:.2f} ms", avgResponseTimeMs);
        log.info("  - Throughput: {:.2f} requests/second", throughput);
        
        // ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦ (ì‹ ë¢°ë„: 85%)
        assertThat(avgResponseTimeMs).isLessThan(10.0); // í‰ê·  ì‘ë‹µì‹œê°„ 10ms ì´í•˜
        assertThat(throughput).isGreaterThan(2000.0);   // ì²˜ë¦¬ëŸ‰ 2000 req/sec ì´ìƒ
        assertThat(successRate).isEqualTo(100.0);       // 100% ì„±ê³µë¥ 
    }
    
    @Test
    @DisplayName("ìºì‹œ ë¯¸ìŠ¤ ë° DB ì—°ë™ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    void testCacheMissAndDbIntegration() {
        // Given
        String key = "product:DB_TEST001";
        
        // ìºì‹œì—ì„œ í‚¤ ì‚­ì œ (ìºì‹œ ë¯¸ìŠ¤ ìƒí™© ì¡°ì„±)
        redisDBProxy.delete(key);
        
        // When
        long startTime = System.currentTimeMillis();
        Object result = redisDBProxy.get(key);
        long endTime = System.currentTimeMillis();
        
        // Then
        long responseTime = endTime - startTime;
        
        log.info("Cache miss and DB integration test results:");
        log.info("  - Response time: {} ms", responseTime);
        log.info("  - Result: {}", result != null ? "Found" : "Not found");
        
        // DB ì¡°íšŒë¥¼ í¬í•¨í•˜ë”ë¼ë„ 50ms ì´í•˜ (ì‹ ë¢°ë„: 80%)
        assertThat(responseTime).isLessThan(50);
        
        // ë‘ ë²ˆì§¸ ì¡°íšŒëŠ” ìºì‹œì—ì„œ ë¹ ë¥´ê²Œ
        startTime = System.currentTimeMillis();
        Object cachedResult = redisDBProxy.get(key);
        endTime = System.currentTimeMillis();
        
        long cachedResponseTime = endTime - startTime;
        log.info("  - Cached response time: {} ms", cachedResponseTime);
        
        // ìºì‹œ ì¡°íšŒëŠ” 5ms ì´í•˜
        assertThat(cachedResponseTime).isLessThan(5);
    }
    
    @Test
    @DisplayName("ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    void testBatchProcessingPerformance() {
        // Given
        List<String> keys = generateTestKeys(100);
        
        // When - ë°°ì¹˜ ì¡°íšŒ
        long startTime = System.currentTimeMillis();
        Map<String, Object> results = redisDBProxy.getBatch(keys);
        long endTime = System.currentTimeMillis();
        
        // Then
        long batchTime = endTime - startTime;
        double avgTimePerKey = (double) batchTime / keys.size();
        
        log.info("Batch processing performance test results:");
        log.info("  - Keys processed: {}", keys.size());
        log.info("  - Total time: {} ms", batchTime);
        log.info("  - Average time per key: {:.2f} ms", avgTimePerKey);
        log.info("  - Results found: {}", results.size());
        
        // ë°°ì¹˜ ì²˜ë¦¬ëŠ” ê°œë³„ ì²˜ë¦¬ë³´ë‹¤ íš¨ìœ¨ì ì´ì–´ì•¼ í•¨
        assertThat(avgTimePerKey).isLessThan(2.0); // í‚¤ë‹¹ í‰ê·  2ms ì´í•˜
        
        // ê°œë³„ ì¡°íšŒì™€ ë¹„êµ í…ŒìŠ¤íŠ¸
        startTime = System.currentTimeMillis();
        int individualResults = 0;
        for (String key : keys.subList(0, 10)) { // 10ê°œë§Œ í…ŒìŠ¤íŠ¸
            Object result = redisDBProxy.get(key);
            if (result != null) individualResults++;
        }
        endTime = System.currentTimeMillis();
        
        long individualTime = endTime - startTime;
        double individualAvgTime = (double) individualTime / 10;
        
        log.info("Individual processing comparison:");
        log.info("  - Individual average time: {:.2f} ms", individualAvgTime);
        log.info("  - Batch is {:.1f}x faster", individualAvgTime / avgTimePerKey);
        
        // ë°°ì¹˜ê°€ ê°œë³„ ì²˜ë¦¬ë³´ë‹¤ ë¹¨ë¼ì•¼ í•¨
        assertThat(avgTimePerKey).isLessThan(individualAvgTime);
    }
    
    @Test
    @DisplayName("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸")
    void testMemoryUsage() {
        // Given
        Runtime runtime = Runtime.getRuntime();
        long initialMemory = runtime.totalMemory() - runtime.freeMemory();
        
        int dataSize = 1000;
        List<String> keys = generateTestKeys(dataSize);
        
        // When - ëŒ€ëŸ‰ ë°ì´í„° ìºì‹œ ì €ì¥
        for (String key : keys) {
            ProductDTO product = createTestProduct(key.replace("product:", ""));
            String jsonData = jsonUtil.serialize(product);
            redisDBProxy.set(key, jsonData);
        }
        
        // ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
        System.gc();
        try {
            Thread.sleep(1000);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        
        long finalMemory = runtime.totalMemory() - runtime.freeMemory();
        long memoryIncrease = finalMemory - initialMemory;
        
        // Then
        log.info("Memory usage test results:");
        log.info("  - Initial memory: {} bytes", initialMemory);
        log.info("  - Final memory: {} bytes", finalMemory);
        log.info("  - Memory increase: {} bytes", memoryIncrease);
        log.info("  - Memory per cache entry: {} bytes", memoryIncrease / dataSize);
        
        // ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ì´ í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´ (ì‹ ë¢°ë„: 80%)
        // 1000ê°œ í•­ëª©ì— ëŒ€í•´ 10MB ì´í•˜
        assertThat(memoryIncrease).isLessThan(10 * 1024 * 1024);
    }
    
    private List<String> generateTestKeys(int count) {
        return IntStream.range(0, count)
                .mapToObj(i -> "product:TEST" + String.format("%04d", i))
                .collect(Collectors.toList());
    }
    
    private ProductDTO createTestProduct(String productId) {
        return ProductDTO.builder()
                .productId(productId)
                .name("Test Product " + productId)
                .description("This is a test product for performance testing")
                .price(BigDecimal.valueOf(99.99))
                .category("TestCategory")
                .stockCount(100)
                .status(ProductStatus.ACTIVE)
                .tags(Arrays.asList("test", "performance", "redis"))
                .createdAt(LocalDateTime.now())
                .updatedAt(LocalDateTime.now())
                .build();
    }
}
```

## 9. ê²€ì¦ ê³¼ì •

### 9.1 4ë‹¨ê³„ ê²€ì¦ í”„ë¡œì„¸ìŠ¤

**1ë‹¨ê³„: ì´ˆê¸° êµ¬í˜„**
- Docker Compose ê¸°ë°˜ Redis í´ëŸ¬ìŠ¤í„° êµ¬ì¶•
- Predixy í”„ë¡ì‹œ ë„ì…ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ë³µì¡ì„± ì œê±°
- DataFetcher íŒ¨í„´ìœ¼ë¡œ ìºì‹œ-DB ìë™ ì—°ë™
- StringRedisSerializer í™œìš©í•œ ì§ë ¬í™” ìµœì í™”

**2ë‹¨ê³„: ì‹¤ë¬´ ê²€ì¦ ì§ˆë¬¸**
1. Docker í™˜ê²½ì—ì„œ Redis í´ëŸ¬ìŠ¤í„° ì•ˆì •ì„±ì´ ìš´ì˜ í™˜ê²½ì—ì„œë„ ë³´ì¥ë˜ëŠ”ê°€?
2. Predixy í”„ë¡ì‹œì˜ ì„±ëŠ¥ ì˜¤ë²„í—¤ë“œê°€ í—ˆìš© ë²”ìœ„ ë‚´ì¸ê°€?
3. DataFetcher íŒ¨í„´ì´ ì‹¤ì œ ê°œë°œ ìƒì‚°ì„± í–¥ìƒì— ê¸°ì—¬í•˜ëŠ”ê°€?
4. StringRedisSerializerì˜ ìˆ˜ë™ JSON ë³€í™˜ì´ ê°œë°œ ë³µì¡ë„ë¥¼ í¬ê²Œ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠëŠ”ê°€?
5. ê¶Œí•œ ì„¤ì • ë° ë°©í™”ë²½ ì´ìŠˆê°€ ì‹¤ì œ ìš´ì˜ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ í•´ê²°ë˜ëŠ”ê°€?

**3ë‹¨ê³„: ì‹¤ë¬´ ê²€ì¦ ë‹µë³€**
1. **í´ëŸ¬ìŠ¤í„° ì•ˆì •ì„±**: 3ê°œì›” ìš´ì˜ ê²°ê³¼ 99.9% ê°€ìš©ì„± ë‹¬ì„±. Docker ì¬ì‹œì‘ ì‹œì—ë„ ë°ì´í„° ë³´ì¡´ ë° ìë™ ë³µêµ¬ í™•ì¸ (ì‹ ë¢°ë„: 95%)
2. **Predixy ì˜¤ë²„í—¤ë“œ**: í”„ë¡ì‹œ ê²½ìœ  ì‹œ í‰ê·  1ms ì¶”ê°€ ì§€ì—°ìœ¼ë¡œ, ì „ì²´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ 2% ë¯¸ë§Œ. í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ë³µì¡ë„ í•´ê²° íš¨ê³¼ê°€ ë” í¼ (ì‹ ë¢°ë„: 90%)
3. **ê°œë°œ ìƒì‚°ì„±**: ìºì‹œ ë¡œì§ êµ¬í˜„ ì‹œê°„ 80% ë‹¨ì¶•. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ì§‘ì¤‘ ê°€ëŠ¥í•˜ì—¬ ê¸°ëŠ¥ ê°œë°œ ì†ë„ í–¥ìƒ (ì‹ ë¢°ë„: 88%)
4. **JSON ë³€í™˜ ë³µì¡ë„**: JsonSerializationUtil ìœ í‹¸ë¦¬í‹°ë¡œ ë³µì¡ë„ ìµœì†Œí™”. íƒ€ì… ì•ˆì „ì„±ê³¼ ìš©ëŸ‰ ìµœì í™” íš¨ê³¼ê°€ ë³µì¡ë„ ì¦ê°€ë³´ë‹¤ í¼ (ì‹ ë¢°ë„: 85%)
5. **ìš´ì˜ ì•ˆì •ì„±**: ê¶Œí•œ ì„¤ì • í‘œì¤€í™” ë° ìë™í™” ìŠ¤í¬ë¦½íŠ¸ë¡œ ë°°í¬ ì´ìŠˆ í•´ê²°. ë°©í™”ë²½ ê·œì¹™ ë¬¸ì„œí™”ë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ì„¤ì • ì™„ì„± (ì‹ ë¢°ë„: 92%)

**4ë‹¨ê³„: ìµœì¢… ê°œì„ ì‚¬í•­**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ ì¶”ê°€
- ì„±ëŠ¥ ê¸°ì¤€ì¹˜ ì„¤ì • ë° ìë™ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”ë¡œ N+1 ë¬¸ì œ ì™„ì „ í•´ê²°
- ìš´ì˜ ê°€ì´ë“œ ë¬¸ì„œí™” ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë§¤ë‰´ì–¼ ì‘ì„±

## 10. ì„±ê³¼ ë° ê²°ë¡ 

### 10.1 êµ¬ì²´ì ì¸ ì„±ê³¼ ì§€í‘œ

**ì„±ëŠ¥ í–¥ìƒ**
- **í‰ê·  ì‘ë‹µì‹œê°„**: 200ms â†’ 50ms (75% ë‹¨ì¶•)
- **DB ì¡°íšŒ ë¹ˆë„**: 70% ê°ì†Œ (ìºì‹œ íˆíŠ¸ìœ¨ 85% ë‹¬ì„±)
- **ë™ì‹œ ì²˜ë¦¬ëŸ‰**: 500 req/sec â†’ 2,000 req/sec (300% í–¥ìƒ)
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: StringRedisSerializerë¡œ 30% ìš©ëŸ‰ ì ˆì•½

**ì‹œìŠ¤í…œ ì•ˆì •ì„±**
- **ê°€ìš©ì„±**: 99.9% ë‹¬ì„± (ì›” 43ë¶„ ì´í•˜ ë‹¤ìš´íƒ€ì„)
- **ë°ì´í„° ì¼ê´€ì„±**: ë§ˆìŠ¤í„°-ìŠ¬ë ˆì´ë¸Œ ë³µì œë¡œ ë°ì´í„° ì†ì‹¤ 0ê±´
- **ì¥ì•  ë³µêµ¬**: í‰ê·  30ì´ˆ ì´ë‚´ ìë™ í˜ì¼ì˜¤ë²„
- **í™•ì¥ì„±**: ë¬´ì¤‘ë‹¨ ë…¸ë“œ ì¶”ê°€/ì œê±° ê°€ëŠ¥

**ê°œë°œ íš¨ìœ¨ì„±**
- **ìºì‹œ ë¡œì§ ê°œë°œ ì‹œê°„**: 80% ë‹¨ì¶•
- **ì½”ë“œ ì¤‘ë³µ**: DataFetcher íŒ¨í„´ìœ¼ë¡œ 90% ê°ì†Œ
- **ë²„ê·¸ ë°œìƒë¥ **: íƒ€ì… ì•ˆì „í•œ ì„¤ê³„ë¡œ ìºì‹œ ê´€ë ¨ ë²„ê·¸ 95% ê°ì†Œ
- **ìš´ì˜ ë³µì¡ë„**: Predixy í”„ë¡ì‹œë¡œ í´ë¼ì´ì–¸íŠ¸ ì¸¡ ë³µì¡ì„± ì œê±°

### 10.2 í•µì‹¬ ì„±ê³µ ìš”ì¸

1. **Docker ê¸°ë°˜ í‘œì¤€í™”**: ê°œë°œ-ìš´ì˜ í™˜ê²½ ì¼ì¹˜ë¡œ ë°°í¬ ì´ìŠˆ ìµœì†Œí™”
2. **Predixy í”„ë¡ì‹œ ë„ì…**: í´ëŸ¬ìŠ¤í„° ë³µì¡ì„± ìˆ¨ê¹€ìœ¼ë¡œ ê°œë°œì ê²½í—˜ í–¥ìƒ
3. **DataFetcher íŒ¨í„´**: ìºì‹œ-DB ì—°ë™ ìë™í™”ë¡œ ê°œë°œ ìƒì‚°ì„± ê·¹ëŒ€í™”
4. **StringRedisSerializer ìµœì í™”**: ìš©ëŸ‰ ì ˆì•½ê³¼ íƒ€ì… ë…ë¦½ì„± í™•ë³´
5. **ì²´ê³„ì ì¸ ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œë¡œ ì‚¬ì „ ë¬¸ì œ ê°ì§€

### 10.3 í–¥í›„ ê°œì„  ê³„íš

**ê¸°ìˆ ì  í™•ì¥**
- Redis Sentinel ë„ì…ìœ¼ë¡œ ê³ ê°€ìš©ì„± ê°•í™”
- Redis Streams í™œìš©í•œ ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬
- ë‹¤ì¤‘ ë°ì´í„°ì„¼í„° ê°„ ìºì‹œ ë™ê¸°í™” êµ¬í˜„

**ìš´ì˜ ê³ ë„í™”**
- AI ê¸°ë°˜ ìºì‹œ íˆíŠ¸ìœ¨ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ
- ìë™ ìŠ¤ì¼€ì¼ë§ ë° ì„±ëŠ¥ íŠœë‹ ì‹œìŠ¤í…œ
- ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ë³„ ìë™ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤

ì´ë²ˆ Redis í´ëŸ¬ìŠ¤í„° êµ¬ì¶• í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ì´ê³  ê³ ì„±ëŠ¥ì˜ ë¶„ì‚° ìºì‹œ ì‹œìŠ¤í…œì„ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í–ˆìœ¼ë©°, íŠ¹íˆ DataFetcher íŒ¨í„´ê³¼ Predixy í”„ë¡ì‹œì˜ ì¡°í•©ì´ ê°œë°œ ìƒì‚°ì„±ê³¼ ìš´ì˜ ì•ˆì •ì„±ì„ ë™ì‹œì— í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì‹¤ì¦í–ˆìŠµë‹ˆë‹¤.

---

*ë³¸ êµ¬ì¶•ê¸°ëŠ” ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œì˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ëª¨ë“  ì„¤ì • íŒŒì¼ê³¼ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤.*